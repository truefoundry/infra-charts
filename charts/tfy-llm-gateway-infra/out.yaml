---
# Source: tfy-llm-gateway-infra/charts/nats/templates/pod-disruption-budget.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels: {}
  name: release-name-nats
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: nats
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: nats
---
# Source: tfy-llm-gateway-infra/charts/altinity-clickhouse-operator/templates/generated/ServiceAccount-clickhouse-operator.yaml
# Template Parameters:
#
# COMMENT=
# NAMESPACE=kube-system
# NAME=clickhouse-operator
#
# Setup ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-altinity-clickhouse-operator
  namespace: default
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.6
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.23.6"
    app.kubernetes.io/managed-by: Helm
  annotations: 
    {}

# Template Parameters:
#
# NAMESPACE=kube-system
# COMMENT=#
# ROLE_KIND=ClusterRole
# ROLE_NAME=clickhouse-operator-kube-system
# ROLE_BINDING_KIND=ClusterRoleBinding
# ROLE_BINDING_NAME=clickhouse-operator-kube-system
#
---
# Source: tfy-llm-gateway-infra/charts/altinity-clickhouse-operator/templates/generated/Secret-clickhouse-operator.yaml
#
# Template parameters available:
#   NAMESPACE=kube-system
#   COMMENT=
#   OPERATOR_VERSION=0.23.6
#   CH_USERNAME_SECRET_PLAIN=clickhouse_operator
#   CH_PASSWORD_SECRET_PLAIN=clickhouse_operator_password
#
apiVersion: v1
kind: Secret
metadata:
  name: release-name-altinity-clickhouse-operator
  namespace: default
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.6
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.23.6"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  username: Y2xpY2tob3VzZV9vcGVyYXRvcg==
  password: Y2xpY2tob3VzZV9vcGVyYXRvcl9wYXNzd29yZA==
---
# Source: tfy-llm-gateway-infra/charts/altinity-clickhouse-operator/templates/generated/ConfigMap-etc-clickhouse-operator-confd-files.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-confd-files
# NAMESPACE=kube-system
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-altinity-clickhouse-operator-confd-files
  namespace: default
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.6
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.23.6"
    app.kubernetes.io/managed-by: Helm
data: 
  null
---
# Source: tfy-llm-gateway-infra/charts/altinity-clickhouse-operator/templates/generated/ConfigMap-etc-clickhouse-operator-configd-files.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-configd-files
# NAMESPACE=kube-system
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-altinity-clickhouse-operator-configd-files
  namespace: default
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.6
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.23.6"
    app.kubernetes.io/managed-by: Helm
data: 
  01-clickhouse-01-listen.xml: |
    <!-- IMPORTANT -->
    <!-- This file is auto-generated -->
    <!-- Do not edit this file - all changes would be lost -->
    <!-- Edit appropriate template in the following folder: -->
    <!-- deploy/builder/templates-config -->
    <!-- IMPORTANT -->
    <yandex>
        <!-- Listen wildcard address to allow accepting connections from other containers and host network. -->
        <listen_host>::</listen_host>
        <listen_host>0.0.0.0</listen_host>
        <listen_try>1</listen_try>
    </yandex>
  01-clickhouse-02-logger.xml: |
    <!-- IMPORTANT -->
    <!-- This file is auto-generated -->
    <!-- Do not edit this file - all changes would be lost -->
    <!-- Edit appropriate template in the following folder: -->
    <!-- deploy/builder/templates-config -->
    <!-- IMPORTANT -->
    <yandex>
        <logger>
            <!-- Possible levels: https://github.com/pocoproject/poco/blob/devel/Foundation/include/Poco/Logger.h#L439 -->
            <level>debug</level>
            <log>/var/log/clickhouse-server/clickhouse-server.log</log>
            <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
            <size>1000M</size>
            <count>10</count>
            <!-- Default behavior is autodetection (log to console if not daemon mode and is tty) -->
            <console>1</console>
        </logger>
    </yandex>
  01-clickhouse-03-query_log.xml: |
    <!-- IMPORTANT -->
    <!-- This file is auto-generated -->
    <!-- Do not edit this file - all changes would be lost -->
    <!-- Edit appropriate template in the following folder: -->
    <!-- deploy/builder/templates-config -->
    <!-- IMPORTANT -->
    <yandex>
        <query_log replace="1">
            <database>system</database>
            <table>query_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_log>
        <query_thread_log remove="1"/>
    </yandex>
  01-clickhouse-04-part_log.xml: |
    <!-- IMPORTANT -->
    <!-- This file is auto-generated -->
    <!-- Do not edit this file - all changes would be lost -->
    <!-- Edit appropriate template in the following folder: -->
    <!-- deploy/builder/templates-config -->
    <!-- IMPORTANT -->
    <yandex>
        <part_log replace="1">
            <database>system</database>
            <table>part_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </part_log>
    </yandex>
  01-clickhouse-05-trace_log.xml: |-
    <!-- IMPORTANT -->
    <!-- This file is auto-generated -->
    <!-- Do not edit this file - all changes would be lost -->
    <!-- Edit appropriate template in the following folder: -->
    <!-- deploy/builder/templates-config -->
    <!-- IMPORTANT -->
    <yandex>
        <trace_log replace="1">
            <database>system</database>
            <table>trace_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </trace_log>
    </yandex>
---
# Source: tfy-llm-gateway-infra/charts/altinity-clickhouse-operator/templates/generated/ConfigMap-etc-clickhouse-operator-files.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-files
# NAMESPACE=kube-system
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-altinity-clickhouse-operator-files
  namespace: default
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.6
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.23.6"
    app.kubernetes.io/managed-by: Helm
data: 
  config.yaml: |-
    annotation:
      exclude: []
      include: []
    clickhouse:
      access:
        password: ""
        port: 8123
        rootCA: ""
        scheme: auto
        secret:
          name: 'release-name-altinity-clickhouse-operator'
          namespace: ""
        timeouts:
          connect: 1
          query: 4
        username: ""
      configuration:
        file:
          path:
            common: config.d
            host: conf.d
            user: users.d
        network:
          hostRegexpTemplate: (chi-{chi}-[^.]+\d+-\d+|clickhouse\-{chi})\.{namespace}\.svc\.cluster\.local$
        user:
          default:
            networksIP:
            - ::1
            - 127.0.0.1
            password: default
            profile: default
            quota: default
      configurationRestartPolicy:
        rules:
        - rules:
          - settings/*: "yes"
          - settings/access_control_path: "no"
          - settings/dictionaries_config: "no"
          - settings/max_server_memory_*: "no"
          - settings/max_*_to_drop: "no"
          - settings/max_concurrent_queries: "no"
          - settings/models_config: "no"
          - settings/user_defined_executable_functions_config: "no"
          - settings/logger/*: "no"
          - settings/macros/*: "no"
          - settings/remote_servers/*: "no"
          - settings/user_directories/*: "no"
          - zookeeper/*: "yes"
          - files/*.xml: "yes"
          - files/config.d/*.xml: "yes"
          - files/config.d/*dict*.xml: "no"
          - profiles/default/background_*_pool_size: "yes"
          - profiles/default/max_*_for_server: "yes"
          version: '*'
        - rules:
          - settings/logger: "yes"
          version: 21.*
      metrics:
        timeouts:
          collect: 9
    label:
      appendScope: "no"
      exclude: []
      include: []
    logger:
      alsologtostderr: "false"
      log_backtrace_at: ""
      logtostderr: "true"
      stderrthreshold: ""
      v: "1"
      vmodule: ""
    pod:
      terminationGracePeriod: 30
    reconcile:
      host:
        wait:
          exclude: true
          include: false
          queries: true
      runtime:
        reconcileCHIsThreadsNumber: 10
        reconcileShardsMaxConcurrencyPercent: 50
        reconcileShardsThreadsNumber: 5
      statefulSet:
        create:
          onFailure: ignore
        update:
          onFailure: abort
          pollInterval: 5
          timeout: 300
    statefulSet:
      revisionHistoryLimit: 0
    template:
      chi:
        path: templates.d
        policy: ApplyOnNextReconcile
    watch:
      namespaces: []
---
# Source: tfy-llm-gateway-infra/charts/altinity-clickhouse-operator/templates/generated/ConfigMap-etc-clickhouse-operator-templatesd-files.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-templatesd-files
# NAMESPACE=kube-system
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-altinity-clickhouse-operator-templatesd-files
  namespace: default
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.6
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.23.6"
    app.kubernetes.io/managed-by: Helm
data: 
  001-templates.json.example: |
    {
      "apiVersion": "clickhouse.altinity.com/v1",
      "kind": "ClickHouseInstallationTemplate",
      "metadata": {
        "name": "01-default-volumeclaimtemplate"
      },
      "spec": {
        "templates": {
          "volumeClaimTemplates": [
            {
              "name": "chi-default-volume-claim-template",
              "spec": {
                "accessModes": [
                  "ReadWriteOnce"
                ],
                "resources": {
                  "requests": {
                    "storage": "2Gi"
                  }
                }
              }
            }
          ],
          "podTemplates": [
            {
              "name": "chi-default-oneperhost-pod-template",
              "distribution": "OnePerHost",
              "spec": {
                "containers" : [
                  {
                    "name": "clickhouse",
                    "image": "clickhouse/clickhouse-server:23.8",
                    "ports": [
                      {
                        "name": "http",
                        "containerPort": 8123
                      },
                      {
                        "name": "client",
                        "containerPort": 9000
                      },
                      {
                        "name": "interserver",
                        "containerPort": 9009
                      }
                    ]
                  }
                ]
              }
            }
          ]
        }
      }
    }
  default-pod-template.yaml.example: |
    apiVersion: "clickhouse.altinity.com/v1"
    kind: "ClickHouseInstallationTemplate"
    metadata:
      name: "default-oneperhost-pod-template"
    spec:
      templates:
        podTemplates:
          - name: default-oneperhost-pod-template
            distribution: "OnePerHost"
  default-storage-template.yaml.example: |
    apiVersion: "clickhouse.altinity.com/v1"
    kind: "ClickHouseInstallationTemplate"
    metadata:
      name: "default-storage-template-2Gi"
    spec:
      templates:
        volumeClaimTemplates:
          - name: default-storage-template-2Gi
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 2Gi
  readme: Templates in this folder are packaged with an operator and available via 'useTemplate'
---
# Source: tfy-llm-gateway-infra/charts/altinity-clickhouse-operator/templates/generated/ConfigMap-etc-clickhouse-operator-usersd-files.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-usersd-files
# NAMESPACE=kube-system
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-altinity-clickhouse-operator-usersd-files
  namespace: default
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.6
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.23.6"
    app.kubernetes.io/managed-by: Helm
data: 
  01-clickhouse-operator-profile.xml: |
    <!-- IMPORTANT -->
    <!-- This file is auto-generated -->
    <!-- Do not edit this file - all changes would be lost -->
    <!-- Edit appropriate template in the following folder: -->
    <!-- deploy/builder/templates-config -->
    <!-- IMPORTANT -->
    <!--
    #
    # Template parameters available:
    #
    -->
    <yandex>
        <!-- clickhouse-operator user is generated by the operator based on config.yaml in runtime -->
        <profiles>
            <clickhouse_operator>
                <log_queries>0</log_queries>
                <skip_unavailable_shards>1</skip_unavailable_shards>
                <http_connection_timeout>10</http_connection_timeout>
                <max_concurrent_queries_for_all_users>0</max_concurrent_queries_for_all_users>
                <os_thread_priority>0</os_thread_priority>
            </clickhouse_operator>
        </profiles>
    </yandex>
  02-clickhouse-default-profile.xml: |-
    <!-- IMPORTANT -->
    <!-- This file is auto-generated -->
    <!-- Do not edit this file - all changes would be lost -->
    <!-- Edit appropriate template in the following folder: -->
    <!-- deploy/builder/templates-config -->
    <!-- IMPORTANT -->
    <yandex>
      <profiles>
        <default>
          <os_thread_priority>2</os_thread_priority>
          <log_queries>1</log_queries>
          <connect_timeout_with_failover_ms>1000</connect_timeout_with_failover_ms>
          <distributed_aggregation_memory_efficient>1</distributed_aggregation_memory_efficient>
          <parallel_view_processing>1</parallel_view_processing>
          <do_not_merge_across_partitions_select_final>1</do_not_merge_across_partitions_select_final>
          <load_balancing>nearest_hostname</load_balancing>
          <prefer_localhost_replica>0</prefer_localhost_replica>
          <!-- materialize_ttl_recalculate_only>1</materialize_ttl_recalculate_only> 21.10 and above -->
        </default>
      </profiles>
    </yandex>
---
# Source: tfy-llm-gateway-infra/charts/nats/templates/config-map.yaml
apiVersion: v1
data:
  nats.conf: |
    {
      "authorization": {
        "timeout": 5,
        "users": [
          {
            "password": $NATS_ADMIN_PASSWORD,
            "permissions": {
              "publish": [
                ">"
              ],
              "subscribe": [
                ">"
              ]
            },
            "user": "admin"
          },
          {
            "password": $NATS_LLM_GATEWAY_REQUEST_LOGGER_PASSWORD,
            "permissions": {
              "publish": [
                "request-log.>"
              ],
              "subscribe": [
                ">"
              ]
            },
            "user": "llm-gateway-request-logger"
          },
          {
            "password": $NATS_CLICKHOUSE_REQUEST_LOGS_READER_PASSWORD,
            "permissions": {
              "subscribe": [
                "request-log.>"
              ]
            },
            "user": "clickhouse-request-logs-reader"
          }
        ]
      },
      "cluster": {
        "name": "release-name-nats",
        "no_advertise": true,
        "port": 6222,
        "routes": [
          "nats://release-name-nats-0.release-name-nats-headless:6222",
          "nats://release-name-nats-1.release-name-nats-headless:6222",
          "nats://release-name-nats-2.release-name-nats-headless:6222"
        ]
      },
      "http_port": 8222,
      "jetstream": {
        "max_file_store": 10Gi,
        "max_memory_store": 250Mi,
        "store_dir": "/data"
      },
      "lame_duck_duration": "30s",
      "lame_duck_grace_period": "10s",
      "pid_file": "/var/run/nats/nats.pid",
      "port": 4222,
      "server_name": $SERVER_NAME,
      "websocket": {
        "compression": true,
        "no_tls": true,
        "port": 8080
      }
    }
kind: ConfigMap
metadata:
  labels: {}
  name: release-name-nats-config
---
# Source: tfy-llm-gateway-infra/charts/zookeeper/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-zookeeper-scripts
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.1.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
data:
  init-certs.sh: |-
    #!/bin/bash
  setup.sh: |-
    #!/bin/bash

    # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
    # check ZOO_SERVER_ID in persistent volume via myid
    # if not present, set based on POD hostname
    if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
        export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
    else
        HOSTNAME="$(hostname -s)"
        if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
            ORD=${BASH_REMATCH[2]}
            export ZOO_SERVER_ID="$((ORD + 1 ))"
        else
            echo "Failed to get index from hostname $HOST"
            exit 1
        fi
    fi
    exec /entrypoint.sh /run.sh
---
# Source: tfy-llm-gateway-infra/charts/altinity-clickhouse-operator/templates/generated/ClusterRole-clickhouse-operator-kube-system.yaml
# Specifies either
#   ClusterRole
# or
#   Role
# to be bound to ServiceAccount.
# ClusterRole is namespace-less and must have unique name
# Role is namespace-bound
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-altinity-clickhouse-operator
  #namespace: kube-system
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.6
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.23.6"
    app.kubernetes.io/managed-by: Helm
  namespace: default
rules:
  #
  # Core API group
  #
  - apiGroups:
      - ""
    resources:
      - configmaps
      - services
      - persistentvolumeclaims
      - secrets
    verbs:
      - get
      - list
      - patch
      - update
      - watch
      - create
      - delete
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - persistentvolumes
    verbs:
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - patch
      - update
      - watch
      - delete
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
  #
  # apps.* resources
  #
  - apiGroups:
      - apps
    resources:
      - statefulsets
    verbs:
      - get
      - list
      - patch
      - update
      - watch
      - create
      - delete
  - apiGroups:
      - apps
    resources:
      - replicasets
    verbs:
      - get
      - patch
      - update
      - delete
  # The operator deployment personally, identified by name
  - apiGroups:
      - apps
    resources:
      - deployments
    resourceNames:
      - release-name-altinity-clickhouse-operator
    verbs:
      - get
      - patch
      - update
      - delete
  #
  # policy.* resources
  #
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - get
      - list
      - patch
      - update
      - watch
      - create
      - delete
  #
  # apiextensions
  #
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
  # clickhouse - related resources
  - apiGroups:
      - clickhouse.altinity.com
    #
    # The operators specific Custom Resources
    #

    resources:
      - clickhouseinstallations
    verbs:
      - get
      - list
      - watch
      - patch
      - update
      - delete
  - apiGroups:
      - clickhouse.altinity.com
    resources:
      - clickhouseinstallationtemplates
      - clickhouseoperatorconfigurations
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - clickhouse.altinity.com
    resources:
      - clickhouseinstallations/finalizers
      - clickhouseinstallationtemplates/finalizers
      - clickhouseoperatorconfigurations/finalizers
    verbs:
      - update
  - apiGroups:
      - clickhouse.altinity.com
    resources:
      - clickhouseinstallations/status
      - clickhouseinstallationtemplates/status
      - clickhouseoperatorconfigurations/status
    verbs:
      - get
      - update
      - patch
      - create
      - delete
  # clickhouse-keeper - related resources
  - apiGroups:
      - clickhouse-keeper.altinity.com
    resources:
      - clickhousekeeperinstallations
    verbs:
      - get
      - list
      - watch
      - patch
      - update
      - delete
  - apiGroups:
      - clickhouse-keeper.altinity.com
    resources:
      - clickhousekeeperinstallations/finalizers
    verbs:
      - update
  - apiGroups:
      - clickhouse-keeper.altinity.com
    resources:
      - clickhousekeeperinstallations/status
    verbs:
      - get
      - update
      - patch
      - create
      - delete
---
# Source: tfy-llm-gateway-infra/charts/altinity-clickhouse-operator/templates/generated/ClusterRoleBinding-clickhouse-operator-kube-system.yaml
# Specifies either
#   ClusterRoleBinding between ClusterRole and ServiceAccount.
# or
#   RoleBinding between Role and ServiceAccount.
# ClusterRoleBinding is namespace-less and must have unique name
# RoleBinding is namespace-bound
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-altinity-clickhouse-operator
  #namespace: kube-system
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.6
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.23.6"
    app.kubernetes.io/managed-by: Helm
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-altinity-clickhouse-operator
subjects:
  - kind: ServiceAccount
    name: release-name-altinity-clickhouse-operator
    namespace: default
---
# Source: tfy-llm-gateway-infra/charts/altinity-clickhouse-operator/templates/generated/Service-clickhouse-operator-metrics.yaml
# Template Parameters:
#
# NAMESPACE=kube-system
# COMMENT=
#
# Setup ClusterIP Service to provide monitoring metrics for Prometheus
# Service would be created in kubectl-specified namespace
# In order to get access outside of k8s it should be exposed as:
# kubectl --namespace prometheus port-forward service/prometheus 9090
# and point browser to localhost:9090
kind: Service
apiVersion: v1
metadata:
  name: release-name-altinity-clickhouse-operator-metrics
  namespace: default
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.6
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.23.6"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - port: 8888
      name: clickhouse-metrics
    - port: 9999
      name: operator-metrics
  selector: 
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: release-name
---
# Source: tfy-llm-gateway-infra/charts/nats/templates/headless-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels: {}
  name: release-name-nats-headless
spec:
  clusterIP: None
  ports:
  - appProtocol: tcp
    name: nats
    port: 4222
    targetPort: nats
  - appProtocol: http
    name: websocket
    port: 8080
    targetPort: websocket
  - appProtocol: tcp
    name: cluster
    port: 6222
    targetPort: cluster
  - appProtocol: http
    name: monitor
    port: 8222
    targetPort: monitor
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/name: nats
---
# Source: tfy-llm-gateway-infra/charts/nats/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels: {}
  name: release-name-nats
spec:
  ports:
  - appProtocol: tcp
    name: nats
    port: 4222
    targetPort: nats
  - appProtocol: http
    name: websocket
    port: 8080
    targetPort: websocket
  - appProtocol: http
    name: monitor
    port: 8222
    targetPort: monitor
  selector:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/name: nats
---
# Source: tfy-llm-gateway-infra/charts/zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-zookeeper-headless
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.1.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: zookeeper
---
# Source: tfy-llm-gateway-infra/charts/zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-zookeeper
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.1.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: zookeeper
---
# Source: tfy-llm-gateway-infra/charts/altinity-clickhouse-operator/templates/generated/Deployment-clickhouse-operator.yaml
# Template Parameters:
#
# NAMESPACE=kube-system
# COMMENT=
# OPERATOR_IMAGE=altinity/clickhouse-operator:0.23.6
# OPERATOR_IMAGE_PULL_POLICY=Always
# METRICS_EXPORTER_IMAGE=altinity/metrics-exporter:0.23.6
# METRICS_EXPORTER_IMAGE_PULL_POLICY=Always
#
# Setup Deployment for clickhouse-operator
# Deployment would be created in kubectl-specified namespace
kind: Deployment
apiVersion: apps/v1
metadata:
  name: release-name-altinity-clickhouse-operator
  namespace: default
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.6
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.23.6"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels: 
      app.kubernetes.io/name: altinity-clickhouse-operator
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels: 
        helm.sh/chart: altinity-clickhouse-operator-0.23.6
        app.kubernetes.io/name: altinity-clickhouse-operator
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "0.23.6"
        app.kubernetes.io/managed-by: Helm
      annotations:
        
        clickhouse-operator-metrics/port: "9999"
        clickhouse-operator-metrics/scrape: "true"
        prometheus.io/port: "8888"
        prometheus.io/scrape: "true"
        checksum/files: 7171425fb3c4020cfb165e807a959cb3413202944140e424abe11406921c9c37
        checksum/confd-files: e0a89f0d2b74c69511b7ed7a15b48f896ed625987d044e45f7f924a965859be9
        checksum/configd-files: 2d22fe701bc9c5ee8d1b2fd78e5bdfbe07616237d771bdaf47c8d1defac893d0
        checksum/templatesd-files: 1a7bc2928760c1b33b407082be025207ca54041857619c701b18ddf065bdb446
        checksum/usersd-files: 0bc69ee6ebe2f1f21a2b3867f2071edad87ffe1e1530f011f70b97a777c99625
    spec:
      serviceAccountName: release-name-altinity-clickhouse-operator
      volumes:
        - name: etc-clickhouse-operator-folder
          configMap:
            name: release-name-altinity-clickhouse-operator-files
        - name: etc-clickhouse-operator-confd-folder
          configMap:
            name: release-name-altinity-clickhouse-operator-confd-files
        - name: etc-clickhouse-operator-configd-folder
          configMap:
            name: release-name-altinity-clickhouse-operator-configd-files
        - name: etc-clickhouse-operator-templatesd-folder
          configMap:
            name: release-name-altinity-clickhouse-operator-templatesd-files
        - name: etc-clickhouse-operator-usersd-folder
          configMap:
            name: release-name-altinity-clickhouse-operator-usersd-files
      containers:
        - name: altinity-clickhouse-operator
          image: altinity/clickhouse-operator:0.23.7
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: etc-clickhouse-operator-folder
              mountPath: /etc/clickhouse-operator
            - name: etc-clickhouse-operator-confd-folder
              mountPath: /etc/clickhouse-operator/conf.d
            - name: etc-clickhouse-operator-configd-folder
              mountPath: /etc/clickhouse-operator/config.d
            - name: etc-clickhouse-operator-templatesd-folder
              mountPath: /etc/clickhouse-operator/templates.d
            - name: etc-clickhouse-operator-usersd-folder
              mountPath: /etc/clickhouse-operator/users.d
          env:
            # Pod-specific
            # spec.nodeName: ip-172-20-52-62.ec2.internal
            - name: OPERATOR_POD_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # metadata.name: clickhouse-operator-6f87589dbb-ftcsf
            - name: OPERATOR_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            # metadata.namespace: kube-system
            - name: OPERATOR_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            # status.podIP: 100.96.3.2
            - name: OPERATOR_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            # spec.serviceAccount: clickhouse-operator
            # spec.serviceAccountName: clickhouse-operator
            - name: OPERATOR_POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            # Container-specific
            - name: OPERATOR_CONTAINER_CPU_REQUEST
              valueFrom:
                resourceFieldRef:
                  containerName: altinity-clickhouse-operator
                  resource: requests.cpu
            - name: OPERATOR_CONTAINER_CPU_LIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: altinity-clickhouse-operator
                  resource: limits.cpu
            - name: OPERATOR_CONTAINER_MEM_REQUEST
              valueFrom:
                resourceFieldRef:
                  containerName: altinity-clickhouse-operator
                  resource: requests.memory
            - name: OPERATOR_CONTAINER_MEM_LIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: altinity-clickhouse-operator
                  resource: limits.memory
            
          ports:
            - containerPort: 9999
              name: metrics
          resources: 
            {}
          securityContext: 
            {}

        - name: metrics-exporter
          image: altinity/metrics-exporter:0.23.7
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: etc-clickhouse-operator-folder
              mountPath: /etc/clickhouse-operator
            - name: etc-clickhouse-operator-confd-folder
              mountPath: /etc/clickhouse-operator/conf.d
            - name: etc-clickhouse-operator-configd-folder
              mountPath: /etc/clickhouse-operator/config.d
            - name: etc-clickhouse-operator-templatesd-folder
              mountPath: /etc/clickhouse-operator/templates.d
            - name: etc-clickhouse-operator-usersd-folder
              mountPath: /etc/clickhouse-operator/users.d
          env:
            # Pod-specific
            # spec.nodeName: ip-172-20-52-62.ec2.internal
            - name: OPERATOR_POD_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # metadata.name: clickhouse-operator-6f87589dbb-ftcsf
            - name: OPERATOR_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            # metadata.namespace: kube-system
            - name: OPERATOR_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            # status.podIP: 100.96.3.2
            - name: OPERATOR_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            # spec.serviceAccount: clickhouse-operator
            # spec.serviceAccountName: clickhouse-operator
            - name: OPERATOR_POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            # Container-specific
            - name: OPERATOR_CONTAINER_CPU_REQUEST
              valueFrom:
                resourceFieldRef:
                  containerName: altinity-clickhouse-operator
                  resource: requests.cpu
            - name: OPERATOR_CONTAINER_CPU_LIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: altinity-clickhouse-operator
                  resource: limits.cpu
            - name: OPERATOR_CONTAINER_MEM_REQUEST
              valueFrom:
                resourceFieldRef:
                  containerName: altinity-clickhouse-operator
                  resource: requests.memory
            - name: OPERATOR_CONTAINER_MEM_LIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: altinity-clickhouse-operator
                  resource: limits.memory
            
          ports:
            - containerPort: 8888
              name: metrics
          resources: 
            {}
          securityContext: 
            {}

      imagePullSecrets: 
        []
      nodeSelector: 
        {}
      affinity: 
        {}
      tolerations: 
        []
      securityContext: 
        {}
---
# Source: tfy-llm-gateway-infra/charts/nats/templates/stateful-set.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels: {}
  name: release-name-nats
spec:
  podManagementPolicy: Parallel
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/component: nats
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: nats
  serviceName: release-name-nats-headless
  template:
    metadata:
      annotations:
        checksum/config: 3a97eb4995683e8440148be872d4edcf20b20a6d6926406f23c081187cef2364
        prometheus.io/port: "7777"
        prometheus.io/scrape: "true"
      labels: {}
    spec:
      containers:
      - args:
        - --config
        - /etc/nats-config/nats.conf
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_NAME
          value: $(POD_NAME)
        - name: NATS_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              key: NATS_ADMIN_PASSWORD
              name: tfy-llm-gateway-infra-auth
        - name: NATS_LLM_GATEWAY_REQUEST_LOGGER_PASSWORD
          valueFrom:
            secretKeyRef:
              key: NATS_LLM_GATEWAY_REQUEST_LOGGER_PASSWORD
              name: tfy-llm-gateway-infra-auth
        - name: NATS_CLICKHOUSE_REQUEST_LOGS_READER_PASSWORD
          valueFrom:
            secretKeyRef:
              key: NATS_CLICKHOUSE_REQUEST_LOGS_READER_PASSWORD
              name: tfy-llm-gateway-infra-auth
        image: nats:2.10.23-alpine3.21
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - nats-server
              - -sl=ldm=/var/run/nats/nats.pid
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz?js-enabled-only=true
            port: monitor
          initialDelaySeconds: 10
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        name: nats
        ports:
        - containerPort: 4222
          name: nats
        - containerPort: 8080
          name: websocket
        - containerPort: 6222
          name: cluster
        - containerPort: 8222
          name: monitor
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz?js-server-only=true
            port: monitor
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          limits:
            cpu: 100m
            memory: 1000Mi
          requests:
            cpu: 100m
            memory: 500Mi
        startupProbe:
          failureThreshold: 90
          httpGet:
            path: /healthz
            port: monitor
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        volumeMounts:
        - mountPath: /etc/nats-config
          name: config
        - mountPath: /var/run/nats
          name: pid
        - mountPath: /data
          name: release-name-nats-js
      - args:
        - -pid
        - /var/run/nats/nats.pid
        - -config
        - /etc/nats-config/nats.conf
        image: natsio/nats-server-config-reloader:0.14.0
        name: reloader
        volumeMounts:
        - mountPath: /var/run/nats
          name: pid
        - mountPath: /etc/nats-config
          name: config
      - args:
        - -port=7777
        - -connz
        - -routez
        - -subz
        - -varz
        - -prefix=nats
        - -use_internal_server_id
        - -jsz=all
        - http://localhost:8222/
        image: natsio/prometheus-nats-exporter:0.14.0
        imagePullPolicy: IfNotPresent
        name: prom-exporter
        ports:
        - containerPort: 7777
          name: prom-metrics
      enableServiceLinks: false
      shareProcessNamespace: true
      tolerations:
      - effect: NoSchedule
        key: cloud.google.com/gke-spot
        operator: Equal
        value: "true"
      - effect: NoSchedule
        key: kubernetes.azure.com/scalesetpriority
        operator: Equal
        value: spot
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: nats
            app.kubernetes.io/instance: release-name
            app.kubernetes.io/name: nats
        maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
      volumes:
      - configMap:
          name: release-name-nats-config
        name: config
      - emptyDir: {}
        name: pid
  volumeClaimTemplates:
  - metadata:
      name: release-name-nats-js
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
---
# Source: tfy-llm-gateway-infra/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-zookeeper
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.1.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  replicas: 1
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: zookeeper
  serviceName: release-name-zookeeper-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: zookeeper
        helm.sh/chart: zookeeper-11.1.1
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: zookeeper
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: zookeeper
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: zookeeper
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.8.1-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr, ruok"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "0"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: release-name-zookeeper-0.release-name-zookeeper-headless.default.svc.cluster.local:2888:3888::1 
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_ENABLE_QUORUM_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          volumeMounts:
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
        - name: scripts
          configMap:
            name: release-name-zookeeper-scripts
            defaultMode: 0755
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: tfy-llm-gateway-infra/templates/clickhouse-installation.yaml
apiVersion: "clickhouse.altinity.com/v1"
kind: "ClickHouseInstallation"
metadata:
  name: release-name
  annotations:
    argocd.argoproj.io/sync-options: Prune=false,Delete=false
  labels:
    
    {}
spec:
  defaults:
    templates:
      serviceTemplate: svc-template
  configuration:
    clusters:
      - name: clickhouse
        layout:
          shardsCount: 1
          replicasCount: 1
        templates:
          podTemplate: clickhouse-stable
          dataVolumeClaimTemplate: clickhouse-data-volume
          serviceTemplate: svc-template
    zookeeper:
      nodes:
        - host: release-name-zookeeper-headless
          port: 2181
    files:
      config.d/named-collections.xml: |
        <clickhouse>
            <named_collections>
                <nats_config>
                    <nats_url from_env="NATS_URL"/>
                    <nats_subjects from_env="NATS_SUBJECTS"/>
                    <nats_username from_env="NATS_USERNAME"/>
                    <nats_password from_env="NATS_PASSWORD"/>
                    <nats_format from_env="NATS_FORMAT" default="JSONEachRow"/>
                    <date_time_input_format from_env="DATE_TIME_INPUT_FORMAT" default="best_effort"/>
                </nats_config>
            </named_collections>
        </clickhouse>
      config.d/log_rotation.xml: |-
        <clickhouse>
            <logger>
                <level>information</level>
                <log>/var/log/clickhouse-server/clickhouse-server.log</log>
                <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
                <size>100M</size>
                <count>5</count>
                <console>1</console>
            </logger>
        </clickhouse>
      users.d/users.xml: |-
        <clickhouse>
          <users>
              <clickhouse_operator>
                  <networks>
                      <ip>0.0.0.0/0</ip>
                      <ip>::/0</ip>
                  </networks>
              </clickhouse_operator>
              <user>
                  <password from_env="CLICKHOUSE_USER_PASSWORD"/>
                  <profile>default</profile>
                  <quota>default</quota>
                  <networks>
                      <ip>0.0.0.0/0</ip>
                      <ip>::/0</ip>
                  </networks>
              </user>
          </users>
        </clickhouse>
  templates:
    serviceTemplates:
      - name: svc-template
        generateName: clickhouse-{chi}
        spec:
          ports:
            - name: http
              port: 8123
            - name: tcp
              port: 9000
          type: ClusterIP
    podTemplates:
      - name: clickhouse-stable
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: clickhouse.altinity.com/cluster
                          operator: In
                          values:
                            - clickhouse
                    topologyKey: "kubernetes.io/hostname"
          containers:
            - name: clickhouse
              image: clickhouse/clickhouse-server:24.7.3.42-alpine
              env:
                - name: NATS_URL
                  value: "release-name-nats.default.svc.cluster.local:4222"
                - name: NATS_SUBJECTS
                  value: "request-log.>"
                - name: NATS_USERNAME
                  value: "clickhouse-request-logs-reader"
                - name: NATS_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: tfy-llm-gateway-infra-auth
                      key: NATS_CLICKHOUSE_REQUEST_LOGS_READER_PASSWORD
                - name: NATS_FORMAT
                  value: "JSONEachRow"
                - name: DATE_TIME_INPUT_FORMAT
                  value: "best_effort"
                - name: CLICKHOUSE_USER_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: tfy-llm-gateway-infra-auth
                      key: CLICKHOUSE_USER_PASSWORD
              resources:
                requests:
                  memory: 4096Mi
                  cpu: 500m
                  ephemeral-storage: 2048Mi
                limits:
                  memory: 4096Mi
                  cpu: 500m
                  ephemeral-storage: 2048Mi
    volumeClaimTemplates:
      - name: clickhouse-data-volume
        spec:
          storageClassName: 
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 100Gi
