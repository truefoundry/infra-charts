## @section Configuration for the cluster type flags.
##
## @param clusterType.awsEks Flag indicating AWS EKS cluster type.
##
## @param clusterType.gcpGkeStandard Flag indicating GCP GKE Standard cluster type.
##
## @param clusterType.gcpGkeAutopilot Flag indicating GCP GKE Autopilot cluster type.
##
## @param clusterType.azureAks Flag indicating Azure AKS cluster type.
##
## @param clusterType.civoTalos Flag indicating Civo Talos cluster type.
##
clusterType:
  awsEks: false
  gcpGkeStandard: false
  gcpGkeAutopilot: false
  azureAks: false
  civoTalos: false

## @section aws-eks-gpu-operator Configuration for the AWS EKS GPU Operator. This section will only be used when clusterType.awsEks is set to true.
##
aws-eks-gpu-operator:
  ## Node Feature Discovery enabled/disable configuration.
  nfd:
    ## @param aws-eks-gpu-operator.nfd.enabled Enable/Disable node feature discovery.
    enabled: true
  
  ## GPU Feature Discovery configuration.
  gfd:
    ## @param aws-eks-gpu-operator.gfd.enabled Enable/Disable gpu feature discovery.
    enabled: true
  
  ## Operator configuration.
  operator:
    ## Resource configuration for operator requests and limits.
    ## @param aws-eks-gpu-operator.operator.resources.requests.cpu CPU request for the operator.
    ## @param aws-eks-gpu-operator.operator.resources.requests.memory Memory request for the operator.
    ## @param aws-eks-gpu-operator.operator.resources.limits.cpu CPU limit for the operator.
    ## @param aws-eks-gpu-operator.operator.resources.limits.memory Memory limit for the operator.
    resources:
      requests:
        cpu: 10m
        memory: 200Mi
      limits:
        cpu: 50m
        memory: 300Mi

  ## Driver configuration
  driver:
    ## @param aws-eks-gpu-operator.driver.enabled Enable/Disable driver installation.
    enabled: false
    
  ## Toolkit configuration.
  toolkit:
    ## @param aws-eks-gpu-operator.toolkit.enabled Enable/Disable nvidia container toolkit installation.
    enabled: true
    ## @param aws-eks-gpu-operator.toolkit.version Version of the toolkit.
    version: v1.14.3-centos7
    ## @skip aws-eks-gpu-operator.toolkit.env
    env:
      - name: ACCEPT_NVIDIA_VISIBLE_DEVICES_ENVVAR_WHEN_UNPRIVILEGED
        value: 'false'
      - name: ACCEPT_NVIDIA_VISIBLE_DEVICES_AS_VOLUME_MOUNTS
        value: 'true'
  
  ## Device Plugin configuration.
  devicePlugin:
    ## @param aws-eks-gpu-operator.devicePlugin.enabled Enable/Disable nvidia device plugin installation.
    enabled: true
    ## @skip aws-eks-gpu-operator.devicePlugin.env
    env:
      - name: PASS_DEVICE_SPECS
        value: 'true'
      - name: DEVICE_LIST_STRATEGY
        value: volume-mounts
      - name: DEVICE_ID_STRATEGY
        value: index
  
  ## Node Feature Discovery configuration.
  node-feature-discovery:
    ## @param aws-eks-gpu-operator.node-feature-discovery.enableNodeFeatureApi Enable/Disable node feature api in node-feature-discovery.
    enableNodeFeatureApi: true
    master:
      ## @param aws-eks-gpu-operator.node-feature-discovery.master.resources.requests.cpu CPU request for master node feature discovery.
      ## @param aws-eks-gpu-operator.node-feature-discovery.master.resources.requests.memory Memory request for master node feature discovery.
      resources:
        requests:
          cpu: 10m
          memory: 400Mi
    worker:
      ## @param aws-eks-gpu-operator.node-feature-discovery.worker.resources.requests.cpu CPU request for worker node feature discovery.
      ## @param aws-eks-gpu-operator.node-feature-discovery.worker.resources.requests.memory Memory request for worker node feature discovery.
      ## @param aws-eks-gpu-operator.node-feature-discovery.worker.resources.limits.cpu CPU limit for worker node feature discovery.
      ## @param aws-eks-gpu-operator.node-feature-discovery.worker.resources.limits.memory Memory limit for worker node feature discovery.
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
        limits:
          cpu: 50m
          memory: 300Mi
      affinity:
        ## @skip aws-eks-gpu-operator.node-feature-discovery.worker.affinity.nodeAffinity
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node.kubernetes.io/instance-type
                    operator: In
                    values:
                      - p2.xlarge
                      - p2.8xlarge
                      - p2.16xlarge
                      - p3.2xlarge
                      - p3.8xlarge
                      - p3.16xlarge
                      - p3dn.24xlarge
                      - p4d.24xlarge
                      - p4de.24xlarge
                      - p5.48xlarge
                      - g4dn.xlarge
                      - g4dn.2xlarge
                      - g4dn.4xlarge
                      - g4dn.8xlarge
                      - g4dn.16xlarge
                      - g4dn.12xlarge
                      - g4dn.metal
                      - g4dn.xlarge
                      - g5.xlarge
                      - g5.2xlarge
                      - g5.4xlarge
                      - g5.8xlarge
                      - g5.16xlarge
                      - g5.12xlarge
                      - g5.24xlarge
                      - g5.48xlarge
      tolerations:
        ## @skip aws-eks-gpu-operator.node-feature-discovery.worker.tolerations[0]
        - key: nvidia.com/gpu
          effect: NoSchedule
          operator: Exists
    gc:
      ## @param aws-eks-gpu-operator.node-feature-discovery.gc.enable Enable node feature discovery garbage collector.
      enable: true
      ## @param aws-eks-gpu-operator.node-feature-discovery.gc.interval Interval between two garbage collection runs.
      interval: 30m
      ## @param aws-eks-gpu-operator.node-feature-discovery.gc.resources.requests.cpu CPU request for node feature discovery garbage collector.
      ## @param aws-eks-gpu-operator.node-feature-discovery.gc.resources.requests.memory Memory request for node feature discovery garbage collector.
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      affinity:
        ## @skip aws-eks-gpu-operator.node-feature-discovery.gc.affinity.nodeAffinity
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node.kubernetes.io/instance-type
                    operator: NotIn
                    values:
                      - p2.xlarge
                      - p2.8xlarge
                      - p2.16xlarge
                      - p3.2xlarge
                      - p3.8xlarge
                      - p3.16xlarge
                      - p3dn.24xlarge
                      - p4d.24xlarge
                      - p4de.24xlarge
                      - p5.48xlarge
                      - g4dn.xlarge
                      - g4dn.2xlarge
                      - g4dn.4xlarge
                      - g4dn.8xlarge
                      - g4dn.16xlarge
                      - g4dn.12xlarge
                      - g4dn.metal
                      - g4dn.xlarge
                      - g5.xlarge
                      - g5.2xlarge
                      - g5.4xlarge
                      - g5.8xlarge
                      - g5.16xlarge
                      - g5.12xlarge
                      - g5.24xlarge
                      - g5.48xlarge
      tolerations:
        ## @skip aws-eks-gpu-operator.node-feature-discovery.gc.tolerations[0]
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

  ## Daemonsets configuration
  daemonsets:
    ## @param aws-eks-gpu-operator.daemonsets.updateStrategy Update Strategy for Daemonsets - one of ["OnDelete", "RollingUpdate"]
    # This is set to OnDelete to protect against pod failures in case device plugin is unavailable during a kubelet restart (caused by toolkit container restart)
    # The downside being Daemonset will not be updated until it is manually deleted,
    # which is mostly okay for gpu case - effect of toolkit, device plugin daemonsets are limited to the node they run on.
    # If needed, older nodes can be drained to force a newer versions on a newer nodes
    # This will be changed to "RollingUpdate" when newer kubelet versions with allocation issue fix become the norm
    # We would also like to use `daemonsets.rollingUpdate.maxSurge: 1` which is not supported yet
    updateStrategy: "OnDelete"
  
  ## Validator configuration.
  validator:
    plugin:
      ## @skip aws-eks-gpu-operator.validator.plugin.env
      env:
        - name: WITH_WORKLOAD
          value: "false"
  
  ## DCGM configuration
  dcgm:
    ## @param aws-eks-gpu-operator.dcgm.enabled Enabled/Disable standalone DCGM.
    enabled: false
    ## @param aws-eks-gpu-operator.dcgm.resources.requests.cpu CPU request for standalone DCGM container
    ## @param aws-eks-gpu-operator.dcgm.resources.requests.memory Memory request for standalone DCGM container
    ## @param aws-eks-gpu-operator.dcgm.resources.limits.cpu CPU limit for standalone DCGM container
    ## @param aws-eks-gpu-operator.dcgm.resources.limits.memory Memory limit for standalone DCGM container
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 50m
        memory: 400Mi
    
  ## DCGM Exporter configuration.
  dcgmExporter:
    ## @param aws-eks-gpu-operator.dcgmExporter.enabled Enabled/Disable DCGM Exporter.
    enabled: false
    ## @param aws-eks-gpu-operator.dcgmExporter.serviceMonitor.enabled Enable or disable ServiceMonitor for DCGM Exporter.
    serviceMonitor:
      enabled: false
    ## @skip aws-eks-gpu-operator.dcgmExporter.env
    env:
      - name: DCGM_EXPORTER_KUBERNETES_GPU_ID_TYPE
        value: uid
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_COLLECTORS
        value: /etc/dcgm-exporter/dcp-metrics-included.csv
    ## @param aws-eks-gpu-operator.dcgmExporter.resources.requests.cpu CPU request for the DCGM Exporter.
    ## @param aws-eks-gpu-operator.dcgmExporter.resources.requests.memory Memory request for the DCGM Exporter.
    ## @param aws-eks-gpu-operator.dcgmExporter.resources.limits.cpu CPU limit for the DCGM Exporter.
    ## @param aws-eks-gpu-operator.dcgmExporter.resources.limits.memory Memory limit for the DCGM Exporter.
    resources:
      requests:
        cpu: 10m
        memory: 300Mi
      limits:
        cpu: 50m
        memory: 400Mi
    ## @param aws-eks-gpu-operator.dcgmExporter.args Arguments for the DCGM Exporter.
    args: ["-c", "5000"]

  migManager:
    ## @skip aws-eks-gpu-operator.migManager.enabled
    enabled: false


## @section gcp-gke-standard-driver Configuration for the GKE Standard Nvidia Drivers. This section will only be used when clusterType.gcpGkeStandard is set to true.
## The exact driver installed will depend on the COS version (decided by the GKE version)
## usually `default` is the stable older driver version and `latest` is experimental newer driver version
## On some COS versions default and latest will point to the same stable driver verison
## Please see relevant GKE and COS release notes: 
## https://cloud.google.com/container-optimized-os/docs/release-notes
## https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#installing_drivers
## Note: `nvidia-l4` is kept in latest for older GKE versions running COS 105 where latest points to newer compatible driver version. We will remove it and place it back in `default` as older GKE version reaches EOL
gcp-gke-standard-driver:
  ## @param gcp-gke-standard-driver.latest.gkeAccelerators Install latest driver for these GKE accelerators.
  latest:
    gkeAccelerators:
      - nvidia-l4


## @section gcp-gke-standard-dcgm-exporter Configuration for the GCP GKE Standard DCGM Exporter. This section will only be used when clusterType.gcpGkeStandard is set to true.
##
gcp-gke-standard-dcgm-exporter:
  ## Docker image tag for the DCGM Exporter.
  ##
  ## @param gcp-gke-standard-dcgm-exporter.image.tag Docker image tag for the DCGM Exporter.
  ##
  image:
    tag: 3.1.7-3.1.4-ubuntu20.04
  ## Arguments to pass to the DCGM Exporter.
  ##
  ## @param gcp-gke-standard-dcgm-exporter.arguments Arguments for the DCGM Exporter.
  ##
  arguments:
    [
      "-c",
      '"5000"',
      "-f",
      "/etc/dcgm-exporter/dcp-metrics-included.csv",
      "--kubernetes-gpu-id-type",
      "device-name",
    ]
  ## Resource configuration for DCGM Exporter requests and limits.
  ##
  ## @param gcp-gke-standard-dcgm-exporter.resources.requests.cpu CPU request for the DCGM Exporter.
  ## @param gcp-gke-standard-dcgm-exporter.resources.requests.memory Memory request for the DCGM Exporter.
  ## @param gcp-gke-standard-dcgm-exporter.resources.limits.cpu CPU limit for the DCGM Exporter.
  ## @param gcp-gke-standard-dcgm-exporter.resources.limits.memory Memory limit for the DCGM Exporter.
  ##
  resources:
    requests:
      cpu: 10m
      memory: 300Mi
    limits:
      cpu: 50m
      memory: 400Mi
  ## Namespace override for the DCGM Exporter.
  ##
  ## @param gcp-gke-standard-dcgm-exporter.namespaceOverride Namespace override for the DCGM Exporter.
  ##
  namespaceOverride: tfy-gpu-operator
  ## ServiceMonitor configuration for Prometheus monitoring.
  ##
  ## @param gcp-gke-standard-dcgm-exporter.serviceMonitor.enabled Enable or disable ServiceMonitor for DCGM Exporter.
  ##
  serviceMonitor:
    enabled: false
  ## Node affinity configuration for worker nodes with GKE accelerators.
  ##
  affinity:
    ## @skip gcp-gke-standard-dcgm-exporter.affinity.nodeAffinity
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: cloud.google.com/gke-accelerator
                operator: Exists
  ## @skip gcp-gke-standard-dcgm-exporter.tolerations[0]
  ##
  tolerations:
    - operator: "Exists"
  ## @param gcp-gke-standard-dcgm-exporter.mapPodsMetrics Enable mapping of pod metrics.
  ##
  mapPodsMetrics: true
  ## Security context configuration.
  ##
  ## @param gcp-gke-standard-dcgm-exporter.securityContext.privileged Set the container to privileged mode.
  ##
  securityContext:
    privileged: true
  ## @param gcp-gke-standard-dcgm-exporter.priorityClassName Priority class name for the DCGM Exporter.
  ##
  priorityClassName: ""
  ## Additional environment variables for the DCGM Exporter.
  ##
  extraEnv:
    ## @param gcp-gke-standard-dcgm-exporter.extraEnv[0].name Name for the additional environment variable for the DCGM Exporter.
    ## @param gcp-gke-standard-dcgm-exporter.extraEnv[0].value Value for the additional environment variable for the DCGM Exporter.
    - name: NVIDIA_INSTALL_DIR_HOST
      value: /home/kubernetes/bin/nvidia
    ## @skip gcp-gke-standard-dcgm-exporter.extraEnv[1]
    - name: NVIDIA_INSTALL_DIR_CONTAINER
      value: /usr/local/nvidia
    ## @skip gcp-gke-standard-dcgm-exporter.extraEnv[2]
    - name: DCGM_EXPORTER_COLLECTORS
      value: "/etc/dcgm-exporter/dcp-metrics-included.csv"
  ## Additional host volumes for the DCGM Exporter.
  ##
  extraHostVolumes:
    ## @param gcp-gke-standard-dcgm-exporter.extraHostVolumes[0].name Name for the additional host volume for the DCGM Exporter.
    ## @param gcp-gke-standard-dcgm-exporter.extraHostVolumes[0].hostPath Host Path for the additional host volume for the DCGM Exporter.
    - name: dev
      hostPath: "/dev"
    ## @skip gcp-gke-standard-dcgm-exporter.extraHostVolumes[1]
    - name: nvidia-install-dir-host
      hostPath: "/home/kubernetes/bin/nvidia"
    ## @skip gcp-gke-standard-dcgm-exporter.extraHostVolumes[2]
    - name: nvidia-config
      hostPath: "/etc/nvidia"
  ## Additional volume mounts for the DCGM Exporter.
  ##
  extraVolumeMounts:
    ## @param gcp-gke-standard-dcgm-exporter.extraVolumeMounts[0].name Name for the additional volume mounts for the DCGM Exporter.
    ## @param gcp-gke-standard-dcgm-exporter.extraVolumeMounts[0].mountPath Mount Path for the additional volume mounts for the DCGM Exporter.
    - name: dev
      mountPath: /dev
    ## @skip gcp-gke-standard-dcgm-exporter.extraVolumeMounts[1]
    - name: nvidia-install-dir-host
      mountPath: /usr/local/nvidia
    ## @skip gcp-gke-standard-dcgm-exporter.extraVolumeMounts[2]
    - name: nvidia-config
      mountPath: /etc/nvidia


## @section azure-aks-gpu-operator Configuration for the Azure AKS GPU operator. This section will only be used when clusterType.azureAks is set to true.
##
azure-aks-gpu-operator:
  nfd:
    enabled: true

  gfd:
    enabled: true

  operator:
    resources:
      limits:
        cpu: 50m
        memory: 300Mi
      requests:
        cpu: 10m
        memory: 100Mi
    tolerations:
      - key: node-role.kubernetes.io/master
        value: ''
        effect: NoSchedule
        operator: Equal
      - key: node-role.kubernetes.io/control-plane
        value: ''
        effect: NoSchedule
        operator: Equal
      - key: kubernetes.azure.com/scalesetpriority
        value: spot
        effect: NoSchedule
        operator: Equal

  node-feature-discovery:
    gc:
      enable: true
      interval: 30m
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      tolerations:
        - key: nvidia.com/gpu
          effect: NoSchedule
          operator: Exists
        - key: node-role.kubernetes.io/master
          value: ''
          effect: NoSchedule
          operator: Equal
        - key: node-role.kubernetes.io/control-plane
          value: ''
          effect: NoSchedule
          operator: Equal
        - key: kubernetes.azure.com/scalesetpriority
          value: spot
          effect: NoSchedule
          operator: Equal
    master:
      resources:
        requests:
          cpu: 10m
          memory: 200Mi
      tolerations:
        - key: node-role.kubernetes.io/master
          value: ''
          effect: NoSchedule
          operator: Equal
        - key: node-role.kubernetes.io/control-plane
          value: ''
          effect: NoSchedule
          operator: Equal
        - key: kubernetes.azure.com/scalesetpriority
          value: spot
          effect: NoSchedule
          operator: Equal
    worker:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.azure.com/accelerator
                    values:
                      - nvidia
                    operator: In
      resources:
        limits:
          cpu: 50m
          memory: 300Mi
        requests:
          cpu: 10m
          memory: 100Mi
      tolerations:
        - key: nvidia.com/gpu
          effect: NoSchedule
          operator: Exists
        - key: sku
          value: gpu
          effect: NoSchedule
          operator: Equal
        - key: kubernetes.azure.com/scalesetpriority
          value: spot
          effect: NoSchedule
          operator: Equal
    enableNodeFeatureApi: true

  daemonsets:
    tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - key: nvidia.com/gpu
        effect: NoSchedule
        operator: Exists
      - key: sku
        value: gpu
        effect: NoSchedule
        operator: Equal
      - key: kubernetes.azure.com/scalesetpriority
        value: spot
        effect: NoSchedule
        operator: Equal
    updateStrategy: OnDelete
    priorityClassName: system-node-critical

  driver:
    enabled: false

  toolkit:
    enabled: true
    version: v1.14.5-ubuntu20.04
    env:
      - name: ACCEPT_NVIDIA_VISIBLE_DEVICES_ENVVAR_WHEN_UNPRIVILEGED
        value: 'false'
      - name: ACCEPT_NVIDIA_VISIBLE_DEVICES_AS_VOLUME_MOUNTS
        value: 'true'

  devicePlugin:
    enabled: true
    env:
      - name: PASS_DEVICE_SPECS
        value: 'true'
      - name: DEVICE_LIST_STRATEGY
        value: volume-mounts
      - name: DEVICE_ID_STRATEGY
        value: index

  dcgm:
    enabled: false
    resources:
      limits:
        cpu: 50m
        memory: 400Mi
      requests:
        cpu: 10m
        memory: 100Mi

  dcgmExporter:
    enabled: true
    env:
      - name: DCGM_EXPORTER_KUBERNETES_GPU_ID_TYPE
        value: uid
      - name: DCGM_EXPORTER_LISTEN
        value: ':9400'
      - name: DCGM_EXPORTER_KUBERNETES
        value: 'true'
      - name: DCGM_EXPORTER_COLLECTORS
        value: /etc/dcgm-exporter/dcp-metrics-included.csv
    args:
      - '-c'
      - '5000'
    resources:
      limits:
        cpu: 50m
        memory: 400Mi
      requests:
        cpu: 10m
        memory: 100Mi
    serviceMonitor:
      enabled: false

  migManager:
    enabled: false
    env:
      - name: WITH_REBOOT
        value: 'true'


## @section civo-talos-gpu-operator Configuration for the Civo Talos GPU Operator. This section will only be used when clusterType.civoTalos is set to true.
##
civo-talos-gpu-operator:
  ## Node Feature Discovery enabled/disable configuration.
  nfd:
    ## @param civo-talos-gpu-operator.nfd.enabled Enable/Disable node feature discovery.
    enabled: true
  
  ## GPU Feature Discovery configuration.
  gfd:
    ## @param civo-talos-gpu-operator.gfd.enabled Enable/Disable gpu feature discovery.
    enabled: true
  
  ## Operator configuration.
  operator:
    ## Resource configuration for operator requests and limits.
    ## @param civo-talos-gpu-operator.operator.resources.requests.cpu CPU request for the operator.
    ## @param civo-talos-gpu-operator.operator.resources.requests.memory Memory request for the operator.
    ## @param civo-talos-gpu-operator.operator.resources.limits.cpu CPU limit for the operator.
    ## @param civo-talos-gpu-operator.operator.resources.limits.memory Memory limit for the operator.
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 50m
        memory: 300Mi
  
  ## Node Feature Discovery configuration.
  node-feature-discovery:
    ## @param civo-talos-gpu-operator.node-feature-discovery.enableNodeFeatureApi Enable/Disable node feature api in node-feature-discovery.
    enableNodeFeatureApi: true
    master:
      ## @param civo-talos-gpu-operator.node-feature-discovery.master.resources.requests.cpu CPU request for master node feature discovery.
      ## @param civo-talos-gpu-operator.node-feature-discovery.master.resources.requests.memory Memory request for master node feature discovery.
      resources:
        requests:
          cpu: 10m
          memory: 200Mi
    worker:
      ## @param civo-talos-gpu-operator.node-feature-discovery.worker.resources.requests.cpu CPU request for worker node feature discovery.
      ## @param civo-talos-gpu-operator.node-feature-discovery.worker.resources.requests.memory Memory request for worker node feature discovery.
      ## @param civo-talos-gpu-operator.node-feature-discovery.worker.resources.limits.cpu CPU limit for worker node feature discovery.
      ## @param civo-talos-gpu-operator.node-feature-discovery.worker.resources.limits.memory Memory limit for worker node feature discovery.
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
        limits:
          cpu: 50m
          memory: 300Mi
      affinity:
        ## @skip civo-talos-gpu-operator.node-feature-discovery.worker.affinity.nodeAffinity
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node.kubernetes.io/instance-type
                    operator: In
                    values:
                      - g4g.40.kube.small
                      - g4g.40.kube.medium
                      - g4g.40.kube.large
                      - g4g.40.kube.xlarge
                      - g4g.kube.small
                      - g4g.kube.medium
                      - g4g.kube.large
                      - g4g.kube.xlarge
                      - an.g1.l40s.kube.x1
                      - an.g1.l40s.kube.x2
                      - an.g1.l40s.kube.x4
                      - an.g1.l40s.kube.x8
      tolerations:
        ## @skip civo-talos-gpu-operator.node-feature-discovery.worker.tolerations[0]
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
    gc:
      ## @param civo-talos-gpu-operator.node-feature-discovery.gc.enable Enable node feature discovery garbage collector.
      enable: true
      ## @param civo-talos-gpu-operator.node-feature-discovery.gc.interval Interval between two garbage collection runs.
      interval: 30m
      ## @param civo-talos-gpu-operator.node-feature-discovery.gc.resources.requests.cpu CPU request for node feature discovery garbage collector.
      ## @param civo-talos-gpu-operator.node-feature-discovery.gc.resources.requests.memory Memory request for node feature discovery garbage collector.
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      affinity:
        ## @skip civo-talos-gpu-operator.node-feature-discovery.gc.affinity.nodeAffinity
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node.kubernetes.io/instance-type
                    operator: NotIn
                    values:
                      - g4g.40.kube.small
                      - g4g.40.kube.medium
                      - g4g.40.kube.large
                      - g4g.40.kube.xlarge
                      - g4g.kube.small
                      - g4g.kube.medium
                      - g4g.kube.large
                      - g4g.kube.xlarge
                      - an.g1.l40s.kube.x1
                      - an.g1.l40s.kube.x2
                      - an.g1.l40s.kube.x4
                      - an.g1.l40s.kube.x8
      tolerations:
        ## @skip civo-talos-gpu-operator.node-feature-discovery.gc.tolerations[0]
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
    
  ## Driver configuration
  driver:
    ## @param civo-talos-gpu-operator.driver.enabled Enable/Disable driver installation.
    enabled: false
  
  ## Toolkit configuration.
  toolkit:
    ## @param civo-talos-gpu-operator.toolkit.enabled Enable/Disable nvidia container toolkit installation.
    enabled: false
  
  ## Device Plugin configuration.
  devicePlugin:
    ## @param civo-talos-gpu-operator.devicePlugin.enabled Enable/Disable nvidia device plugin installation.
    enabled: true
    ## @skip civo-talos-gpu-operator.devicePlugin.env
    env:
      - name: PASS_DEVICE_SPECS
        value: 'true'
      - name: DEVICE_LIST_STRATEGY
        value: volume-mounts
      - name: DEVICE_ID_STRATEGY
        value: index

  ## DCGM configuration
  dcgm:
    ## @param civo-talos-gpu-operator.dcgm.enabled Enabled/Disable standalone DCGM.
    enabled: true
    ## @param civo-talos-gpu-operator.dcgm.resources.requests.cpu CPU request for standalone DCGM container
    ## @param civo-talos-gpu-operator.dcgm.resources.requests.memory Memory request for standalone DCGM container
    ## @param civo-talos-gpu-operator.dcgm.resources.limits.cpu CPU limit for standalone DCGM container
    ## @param civo-talos-gpu-operator.dcgm.resources.limits.memory Memory limit for standalone DCGM container
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 50m
        memory: 400Mi

  ## DCGM Exporter configuration.
  dcgmExporter:
    ## @param civo-talos-gpu-operator.dcgmExporter.enabled Enabled/Disable DCGM Exporter.
    enabled: true
    ## @param civo-talos-gpu-operator.dcgmExporter.serviceMonitor.enabled Enable or disable ServiceMonitor for DCGM Exporter.
    serviceMonitor:
      enabled: false
    ## @skip civo-talos-gpu-operator.dcgmExporter.env
    env:
      - name: DCGM_EXPORTER_KUBERNETES_GPU_ID_TYPE
        value: uid
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_COLLECTORS
        value: /etc/dcgm-exporter/dcp-metrics-included.csv
    ## @param civo-talos-gpu-operator.dcgmExporter.resources.requests.cpu CPU request for the DCGM Exporter.
    ## @param civo-talos-gpu-operator.dcgmExporter.resources.requests.memory Memory request for the DCGM Exporter.
    ## @param civo-talos-gpu-operator.dcgmExporter.resources.limits.cpu CPU limit for the DCGM Exporter.
    ## @param civo-talos-gpu-operator.dcgmExporter.resources.limits.memory Memory limit for the DCGM Exporter.
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 50m
        memory: 400Mi
    ## @param civo-talos-gpu-operator.dcgmExporter.args Arguments for the DCGM Exporter.
    args: ["-c", "5000"]

  migManager:
    ## @skip civo-talos-gpu-operator.migManager.enabled
    enabled: false
