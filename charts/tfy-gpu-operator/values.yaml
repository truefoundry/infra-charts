## @section Configuration for the cluster type flags.
##
## @param clusterType.awsEks Flag indicating AWS EKS cluster type.
##
## @param clusterType.gcpGkeStandard Flag indicating GCP GKE Standard cluster type.
##
## @param clusterType.azureAks Flag indicating Azure AKS cluster type.
##
## @param clusterType.civoTalos Flag indicating Civo Talos cluster type.
##
## @param clusterType.generic Flag indicating Generic cluster type.
##
clusterType:
  awsEks: false
  gcpGkeStandard: false
  azureAks: false
  civoTalos: false
  generic: false


## @section aws-eks-gpu-operator Configuration for the AWS EKS GPU Operator. This section will only be used when clusterType.awsEks is set to true.
##
aws-eks-gpu-operator:
  ## Node Feature Discovery enabled/disable configuration.
  nfd:
    ## @param aws-eks-gpu-operator.nfd.enabled Enable/Disable node feature discovery.
    enabled: true
  
  ## GPU Feature Discovery configuration.
  gfd:
    ## @param aws-eks-gpu-operator.gfd.enabled Enable/Disable gpu feature discovery.
    enabled: true
  
  ## Operator configuration.
  operator:
    ## @param aws-eks-gpu-operator.operator.upgradeCRD upgrade CRD on chart upgrade
    upgradeCRD: true
    ## @param aws-eks-gpu-operator.operator.cleanupCRD cleanup CRD on chart uninstall
    cleanupCRD: true
    ## Resource configuration for operator requests and limits.
    ## @param aws-eks-gpu-operator.operator.resources.requests.cpu CPU request for the operator.
    ## @param aws-eks-gpu-operator.operator.resources.requests.memory Memory request for the operator.
    ## @param aws-eks-gpu-operator.operator.resources.limits.cpu CPU limit for the operator.
    ## @param aws-eks-gpu-operator.operator.resources.limits.memory Memory limit for the operator.
    resources:
      requests:
        cpu: 10m
        memory: 200Mi
      limits:
        cpu: 50m
        memory: 300Mi

  ## Driver configuration
  driver:
    ## @param aws-eks-gpu-operator.driver.enabled Enable/Disable driver installation.
    enabled: false
    
  ## Toolkit configuration.
  toolkit:
    ## @param aws-eks-gpu-operator.toolkit.enabled Enable/Disable nvidia container toolkit installation.
    enabled: true
    ## @param aws-eks-gpu-operator.toolkit.version Version of the toolkit.
    version: v1.17.2-ubi8
    ## @skip aws-eks-gpu-operator.toolkit.env
    env:
      - name: ACCEPT_NVIDIA_VISIBLE_DEVICES_ENVVAR_WHEN_UNPRIVILEGED
        value: 'false'
      - name: ACCEPT_NVIDIA_VISIBLE_DEVICES_AS_VOLUME_MOUNTS
        value: 'true'
  
  ## Device Plugin configuration.
  devicePlugin:
    ## @param aws-eks-gpu-operator.devicePlugin.enabled Enable/Disable nvidia device plugin installation.
    enabled: true
    ## @skip aws-eks-gpu-operator.devicePlugin.env
    env:
      - name: PASS_DEVICE_SPECS
        value: 'true'
      - name: DEVICE_LIST_STRATEGY
        value: volume-mounts
      - name: DEVICE_ID_STRATEGY
        value: index
    ## @skip aws-eks-gpu-operator.devicePlugin.config
    config:
      data:
        all: ''
        time-sliced-10: |-
          version: v1
          sharing:
            timeSlicing:
              renameByDefault: true
              resources:
              - name: nvidia.com/gpu
                replicas: 10
      name: ''
      create: false
      default: ''
  
  ## Node Feature Discovery configuration.
  node-feature-discovery:
    ## @param aws-eks-gpu-operator.node-feature-discovery.enableNodeFeatureApi Enable/Disable node feature api in node-feature-discovery.
    enableNodeFeatureApi: true
    master:
      ## @param aws-eks-gpu-operator.node-feature-discovery.master.resources.requests.cpu CPU request for master node feature discovery.
      ## @param aws-eks-gpu-operator.node-feature-discovery.master.resources.requests.memory Memory request for master node feature discovery.
      resources:
        requests:
          cpu: 10m
          memory: 400Mi
    worker:
      ## @param aws-eks-gpu-operator.node-feature-discovery.worker.resources.requests.cpu CPU request for worker node feature discovery.
      ## @param aws-eks-gpu-operator.node-feature-discovery.worker.resources.requests.memory Memory request for worker node feature discovery.
      ## @param aws-eks-gpu-operator.node-feature-discovery.worker.resources.limits.cpu CPU limit for worker node feature discovery.
      ## @param aws-eks-gpu-operator.node-feature-discovery.worker.resources.limits.memory Memory limit for worker node feature discovery.
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
        limits:
          cpu: 50m
          memory: 300Mi
      affinity:
        ## @skip aws-eks-gpu-operator.node-feature-discovery.worker.affinity.nodeAffinity
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node.kubernetes.io/instance-type
                    operator: In
                    values:
                      - p2.xlarge
                      - p2.8xlarge
                      - p2.16xlarge
                      - p3.2xlarge
                      - p3.8xlarge
                      - p3.16xlarge
                      - p3dn.24xlarge
                      - p4d.24xlarge
                      - p4de.24xlarge
                      - p5.48xlarge
                      - p5e.48xlarge
                      - g4dn.xlarge
                      - g4dn.2xlarge
                      - g4dn.4xlarge
                      - g4dn.8xlarge
                      - g4dn.16xlarge
                      - g4dn.12xlarge
                      - g4dn.metal
                      - g4dn.xlarge
                      - g5.xlarge
                      - g5.2xlarge
                      - g5.4xlarge
                      - g5.8xlarge
                      - g5.16xlarge
                      - g5.12xlarge
                      - g5.24xlarge
                      - g5.48xlarge
                      - g6.xlarge
                      - g6.2xlarge
                      - g6.4xlarge
                      - g6.8xlarge
                      - g6.16xlarge
                      - g6.12xlarge
                      - g6.24xlarge
                      - g6.48xlarge
                      - g6e.xlarge
                      - g6e.2xlarge
                      - g6e.4xlarge
                      - g6e.8xlarge
                      - g6e.16xlarge
                      - g6e.12xlarge
                      - g6e.24xlarge
                      - g6e.48xlarge
      tolerations:
        ## @skip aws-eks-gpu-operator.node-feature-discovery.worker.tolerations[0]
        - key: nvidia.com/gpu
          effect: NoSchedule
          operator: Exists
    gc:
      ## @param aws-eks-gpu-operator.node-feature-discovery.gc.enable Enable node feature discovery garbage collector.
      enable: true
      ## @param aws-eks-gpu-operator.node-feature-discovery.gc.interval Interval between two garbage collection runs.
      interval: 30m
      ## @param aws-eks-gpu-operator.node-feature-discovery.gc.resources.requests.cpu CPU request for node feature discovery garbage collector.
      ## @param aws-eks-gpu-operator.node-feature-discovery.gc.resources.requests.memory Memory request for node feature discovery garbage collector.
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      affinity:
        ## @skip aws-eks-gpu-operator.node-feature-discovery.gc.affinity.nodeAffinity
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node.kubernetes.io/instance-type
                    operator: NotIn
                    values:
                      - p2.xlarge
                      - p2.8xlarge
                      - p2.16xlarge
                      - p3.2xlarge
                      - p3.8xlarge
                      - p3.16xlarge
                      - p3dn.24xlarge
                      - p4d.24xlarge
                      - p4de.24xlarge
                      - p5.48xlarge
                      - p5e.48xlarge
                      - g4dn.xlarge
                      - g4dn.2xlarge
                      - g4dn.4xlarge
                      - g4dn.8xlarge
                      - g4dn.16xlarge
                      - g4dn.12xlarge
                      - g4dn.metal
                      - g4dn.xlarge
                      - g5.xlarge
                      - g5.2xlarge
                      - g5.4xlarge
                      - g5.8xlarge
                      - g5.16xlarge
                      - g5.12xlarge
                      - g5.24xlarge
                      - g5.48xlarge
                      - g6.xlarge
                      - g6.2xlarge
                      - g6.4xlarge
                      - g6.8xlarge
                      - g6.16xlarge
                      - g6.12xlarge
                      - g6.24xlarge
                      - g6.48xlarge
                      - gr6.4xlarge
                      - gr6.8xlarge
                      - g6e.xlarge
                      - g6e.2xlarge
                      - g6e.4xlarge
                      - g6e.8xlarge
                      - g6e.16xlarge
                      - g6e.12xlarge
                      - g6e.24xlarge
                      - g6e.48xlarge
      tolerations:
        ## @skip aws-eks-gpu-operator.node-feature-discovery.gc.tolerations[0]
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

  ## Daemonsets configuration
  daemonsets:
    ## @param aws-eks-gpu-operator.daemonsets.updateStrategy Update Strategy for Daemonsets - one of ["OnDelete", "RollingUpdate"]
    # This is set to OnDelete to protect against pod failures in case device plugin is unavailable during a kubelet restart (caused by toolkit container restart)
    # The downside being Daemonset will not be updated until it is manually deleted,
    # which is mostly okay for gpu case - effect of toolkit, device plugin daemonsets are limited to the node they run on.
    # If needed, older nodes can be drained to force a newer versions on a newer nodes
    # This will be changed to "RollingUpdate" when newer kubelet versions with allocation issue fix become the norm
    # We would also like to use `daemonsets.rollingUpdate.maxSurge: 1` which is not supported yet
    updateStrategy: "OnDelete"
  
  ## Validator configuration.
  validator:
    cuda:
      ## @skip aws-eks-gpu-operator.validator.cuda.env
      env:
        - name: WITH_WORKLOAD
          value: "false"
    plugin:
      ## @skip aws-eks-gpu-operator.validator.plugin.env
      env:
        - name: WITH_WORKLOAD
          value: "false"
  
  ## DCGM configuration
  dcgm:
    ## @param aws-eks-gpu-operator.dcgm.enabled Enabled/Disable standalone DCGM.
    enabled: false
    ## @param aws-eks-gpu-operator.dcgm.version Image tag for DCGM container. Find all image tags at https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cloud-native/containers/dcgm/tags
    version: 3.3.8-1-ubuntu22.04
    ## @param aws-eks-gpu-operator.dcgm.resources.requests.cpu CPU request for standalone DCGM container
    ## @param aws-eks-gpu-operator.dcgm.resources.requests.memory Memory request for standalone DCGM container
    ## @param aws-eks-gpu-operator.dcgm.resources.limits.cpu CPU limit for standalone DCGM container
    ## @param aws-eks-gpu-operator.dcgm.resources.limits.memory Memory limit for standalone DCGM container
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 50m
        memory: 400Mi
    
  ## DCGM Exporter configuration.
  dcgmExporter:
    ## @param aws-eks-gpu-operator.dcgmExporter.enabled Enabled/Disable DCGM Exporter. Requires tfy-karpenter-config >= 0.18 to operate safely because of Nvidia GSP issues
    enabled: true
    ## @param aws-eks-gpu-operator.dcgmExporter.version Image tag version for DCGM Exporter. Find all tags at https://catalog.ngc.nvidia.com/orgs/nvidia/teams/k8s/containers/dcgm-exporter/tags
    version: 3.3.8-3.6.0-ubuntu22.04
    serviceMonitor:
      ## @param aws-eks-gpu-operator.dcgmExporter.serviceMonitor.enabled Enable or disable ServiceMonitor for DCGM Exporter.
      enabled: false
      ## @skip aws-eks-gpu-operator.dcgmExporter.serviceMonitor.honorLabels
      honorLabels: true
      ## @skip aws-eks-gpu-operator.dcgmExporter.serviceMonitor.additionalLabels
      additionalLabels:
        release: prometheus
      ## @skip aws-eks-gpu-operator.dcgmExporter.serviceMonitor.relabelings
      relabelings:
        - action: replace
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: kubernetes_node
        - action: replace
          targetLabel: job
          replacement: gpu-metrics
    ## @skip aws-eks-gpu-operator.dcgmExporter.env
    env:
      - name: DCGM_EXPORTER_KUBERNETES_GPU_ID_TYPE
        value: uid
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_COLLECTORS
        value: /etc/dcgm-exporter/dcp-metrics-included.csv
    ## @param aws-eks-gpu-operator.dcgmExporter.resources.requests.cpu CPU request for the DCGM Exporter.
    ## @param aws-eks-gpu-operator.dcgmExporter.resources.requests.memory Memory request for the DCGM Exporter.
    ## @param aws-eks-gpu-operator.dcgmExporter.resources.limits.cpu CPU limit for the DCGM Exporter.
    ## @param aws-eks-gpu-operator.dcgmExporter.resources.limits.memory Memory limit for the DCGM Exporter.
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 100m
        memory: 1000Mi
    ## @param aws-eks-gpu-operator.dcgmExporter.args Arguments for the DCGM Exporter.
    args: ["-c", "5000"]

  ## MIG Configuration
  mig:
    ## @param aws-eks-gpu-operator.mig.strategy migStrategy for mig node, single or mixed
    strategy: none

  ## MIG Manager configuration.
  migManager:
    ## @skip aws-eks-gpu-operator.migManager.enabled
    enabled: false


## @section gcp-gke-standard-driver Configuration for the GKE Standard Nvidia Drivers. This section will only be used when clusterType.gcpGkeStandard is set to true.
## The exact driver installed will depend on the COS version (decided by the GKE version)
## usually `default` is the stable older driver version and `latest` is experimental newer driver version
## On some COS versions default and latest will point to the same stable driver verison
## Please see relevant GKE and COS release notes: 
## https://cloud.google.com/container-optimized-os/docs/release-notes
## https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#installing_drivers
## Note: `nvidia-l4` is kept in latest for older GKE versions running COS 105 where latest points to newer compatible driver version. We will remove it and place it back in `default` as older GKE version reaches EOL
gcp-gke-standard-driver:
  ## @param gcp-gke-standard-driver.enabled If to enabled driver installation via this chart. On some clusters the PSP policy might not allow priviledged context which is required for drivers installation - Fortunately, GKE offers automatic driver installation on nodepools and NAP nodes 1.29.2+. In such cases, we recommend setting this to false.
  enabled: true
  ## @param gcp-gke-standard-driver.latest.gkeAccelerators Install latest driver for these GKE accelerators.
  latest:
    gkeAccelerators:
      - nvidia-l4
      - nvidia-h100-80gb
    cosDriverInstaller:
      image:
        ## @param gcp-gke-standard-driver.latest.cosDriverInstaller.image.repository Repository for the Nvidia Latest Driver Installer cos initContainer.
        ##
        repository: "cos-nvidia-installer"
        ## @param gcp-gke-standard-driver.latest.cosDriverInstaller.image.tag Tag for the Nvidia Latest Driver Installer cos initContainer.
        tag: "fixed"
    partitionGpus:
      image:
        ## @param gcp-gke-standard-driver.latest.partitionGpus.image.repository Repository for the Nvidia Latest Driver Installer partition gpus init container.
        ##
        repository: "gcr.io/gke-release/nvidia-partition-gpu@sha256"
        ## @param gcp-gke-standard-driver.latest.partitionGpus.image.tag Tag for the Nvidia Latest Driver Installer partition gpus init container.
        tag: "e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
    pause:
      image:
        ## @param gcp-gke-standard-driver.latest.pause.image.repository Repository for the Nvidia Latest Driver Installer pause container.
        repository: "gke.gcr.io/pause"
        ## @param gcp-gke-standard-driver.latest.pause.image.tag Tag for the Nvidia Latest Driver Installer pause container.
        tag: "3.8@sha256:880e63f94b145e46f1b1082bb71b85e21f16b99b180b9996407d61240ceb9830"
  default:
    cosDriverInstaller:
      image:
        ## @param gcp-gke-standard-driver.default.cosDriverInstaller.image.repository Repository for the Nvidia Driver Installer cos initContainer.
        ##
        repository: "cos-nvidia-installer"
        ## @param gcp-gke-standard-driver.default.cosDriverInstaller.image.tag Tag for the Nvidia Driver Installer cos initContainer.
        ##
        tag: "fixed"
    partitionGpus:
      image:
        ## @param gcp-gke-standard-driver.default.partitionGpus.image.repository Repository for the Nvidia Driver Installer partition gpus init container.
        ##
        repository: "gcr.io/gke-release/nvidia-partition-gpu@sha256"
        ## @param gcp-gke-standard-driver.default.partitionGpus.image.tag Tag for the Nvidia Driver Installer partition gpus init container.
        ##
        tag: "e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
    pause:
      image:
        ## @param gcp-gke-standard-driver.default.pause.image.repository Repository for the Nvidia Driver Installer pause container.
        ##
        repository: "gke.gcr.io/pause"
        ## @param gcp-gke-standard-driver.default.pause.image.tag Tag for the Nvidia Driver Installer pause container.
        ##
        tag: "3.8@sha256:880e63f94b145e46f1b1082bb71b85e21f16b99b180b9996407d61240ceb9830"

## @section gcp-gke-standard-dcgm-exporter Configuration for the GCP GKE Standard DCGM Exporter. This section will only be used when clusterType.gcpGkeStandard is set to true.
##
gcp-gke-standard-dcgm-exporter:
  ## Docker image tag for the DCGM Exporter.
  ##
  ## @param gcp-gke-standard-dcgm-exporter.arguments Arguments for the DCGM Exporter.
  ##
  arguments:
    [
      "-c",
      '"5000"',
      "-f",
      "/etc/dcgm-exporter/dcp-metrics-included.csv",
      "--kubernetes-gpu-id-type",
      "device-name",
    ]
  ## Resource configuration for DCGM Exporter requests and limits.
  ##
  ## @param gcp-gke-standard-dcgm-exporter.resources.requests.cpu CPU request for the DCGM Exporter.
  ## @param gcp-gke-standard-dcgm-exporter.resources.requests.memory Memory request for the DCGM Exporter.
  ## @param gcp-gke-standard-dcgm-exporter.resources.limits.cpu CPU limit for the DCGM Exporter.
  ## @param gcp-gke-standard-dcgm-exporter.resources.limits.memory Memory limit for the DCGM Exporter.
  ##
  resources:
    requests:
      cpu: 10m
      memory: 300Mi
    limits:
      cpu: 50m
      memory: 400Mi
  ## ServiceMonitor configuration for Prometheus monitoring.
  ##
  serviceMonitor:
    ## @param gcp-gke-standard-dcgm-exporter.serviceMonitor.enabled Enable or disable ServiceMonitor for DCGM Exporter.
    enabled: false
    ## @skip gcp-gke-standard-dcgm-exporter.serviceMonitor.honorLabels
    honorLabels: true
    ## @skip gcp-gke-standard-dcgm-exporter.serviceMonitor.additionalLabels
    additionalLabels:
      release: prometheus
    ## @skip gcp-gke-standard-dcgm-exporter.serviceMonitor.relabelings
    relabelings:
      - action: replace
        sourceLabels:
          - __meta_kubernetes_pod_node_name
        targetLabel: kubernetes_node
      - action: replace
        targetLabel: job
        replacement: gpu-metrics
  ## Node affinity configuration for worker nodes with GKE accelerators.
  ##
  affinity:
    ## @skip gcp-gke-standard-dcgm-exporter.affinity.nodeAffinity
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: cloud.google.com/gke-accelerator
                operator: Exists
  ## @skip gcp-gke-standard-dcgm-exporter.tolerations[0]
  ##
  tolerations:
    - operator: "Exists"
  ## @param gcp-gke-standard-dcgm-exporter.mapPodsMetrics Enable mapping of pod metrics.
  ##
  mapPodsMetrics: true
  ## Security context configuration.
  ##
  ## @param gcp-gke-standard-dcgm-exporter.securityContext.privileged Set the container to privileged mode.
  ##
  securityContext:
    privileged: true
  ## @param gcp-gke-standard-dcgm-exporter.priorityClassName Priority class name for the DCGM Exporter.
  ##
  priorityClassName: ""
  ## Additional environment variables for the DCGM Exporter.
  ##
  extraEnv:
    ## @param gcp-gke-standard-dcgm-exporter.extraEnv[0].name Name for the additional environment variable for the DCGM Exporter.
    ## @param gcp-gke-standard-dcgm-exporter.extraEnv[0].value Value for the additional environment variable for the DCGM Exporter.
    - name: NVIDIA_INSTALL_DIR_HOST
      value: /home/kubernetes/bin/nvidia
    ## @skip gcp-gke-standard-dcgm-exporter.extraEnv[1]
    - name: NVIDIA_INSTALL_DIR_CONTAINER
      value: /usr/local/nvidia
    ## @skip gcp-gke-standard-dcgm-exporter.extraEnv[2]
    - name: DCGM_EXPORTER_COLLECTORS
      value: "/etc/dcgm-exporter/dcp-metrics-included.csv"
  ## Additional host volumes for the DCGM Exporter.
  ##
  extraHostVolumes:
    ## @param gcp-gke-standard-dcgm-exporter.extraHostVolumes[0].name Name for the additional host volume for the DCGM Exporter.
    ## @param gcp-gke-standard-dcgm-exporter.extraHostVolumes[0].hostPath Host Path for the additional host volume for the DCGM Exporter.
    - name: dev
      hostPath: "/dev"
    ## @skip gcp-gke-standard-dcgm-exporter.extraHostVolumes[1]
    - name: nvidia-install-dir-host
      hostPath: "/home/kubernetes/bin/nvidia"
    ## @skip gcp-gke-standard-dcgm-exporter.extraHostVolumes[2]
    - name: nvidia-config
      hostPath: "/etc/nvidia"
  ## Additional volume mounts for the DCGM Exporter.
  ##
  extraVolumeMounts:
    ## @param gcp-gke-standard-dcgm-exporter.extraVolumeMounts[0].name Name for the additional volume mounts for the DCGM Exporter.
    ## @param gcp-gke-standard-dcgm-exporter.extraVolumeMounts[0].mountPath Mount Path for the additional volume mounts for the DCGM Exporter.
    - name: dev
      mountPath: /dev
    ## @skip gcp-gke-standard-dcgm-exporter.extraVolumeMounts[1]
    - name: nvidia-install-dir-host
      mountPath: /usr/local/nvidia
    ## @skip gcp-gke-standard-dcgm-exporter.extraVolumeMounts[2]
    - name: nvidia-config
      mountPath: /etc/nvidia

## @section azure-aks-gpu-operator Configuration for the Azure AKS GPU operator. This section will only be used when clusterType.azureAks is set to true.
##
azure-aks-gpu-operator:
  ## Node Feature Discovery enabled/disable configuration.
  nfd:
    ## @param azure-aks-gpu-operator.nfd.enabled Enable/Disable node feature discovery.
    enabled: true
  
  ## GPU Feature Discovery configuration.
  gfd:
    ## @param azure-aks-gpu-operator.gfd.enabled Enable/Disable gpu feature discovery.
    enabled: true
    ## @skip azure-aks-gpu-operator.gfd.env
    env:
      - name: GFD_SLEEP_INTERVAL
        value: 60s
      - name: GFD_FAIL_ON_INIT_ERROR
        value: "true"
  
  ## Operator configuration.
  operator:
    ## @param azure-aks-gpu-operator.operator.upgradeCRD upgrade CRD on chart upgrade
    upgradeCRD: true
    ## @param azure-aks-gpu-operator.operator.cleanupCRD cleanup CRD on chart uninstall
    cleanupCRD: true
    ## Resource configuration for operator requests and limits.
    ## @param azure-aks-gpu-operator.operator.resources.requests.cpu CPU request for the operator.
    ## @param azure-aks-gpu-operator.operator.resources.requests.memory Memory request for the operator.
    ## @param azure-aks-gpu-operator.operator.resources.limits.cpu CPU limit for the operator.
    ## @param azure-aks-gpu-operator.operator.resources.limits.memory Memory limit for the operator.
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 50m
        memory: 300Mi
    ## @skip azure-aks-gpu-operator.operator.tolerations
    tolerations:
      - key: node-role.kubernetes.io/master
        operator: Equal
        value: ''
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Equal
        value: ''
        effect: NoSchedule
      - key: kubernetes.azure.com/scalesetpriority
        operator: Equal
        value: spot
        effect: NoSchedule

  ## Node Feature Discovery configuration.
  node-feature-discovery:
    ## @param azure-aks-gpu-operator.node-feature-discovery.enableNodeFeatureApi Enable/Disable node feature api in node-feature-discovery.
    enableNodeFeatureApi: true
    master:
      ## @param azure-aks-gpu-operator.node-feature-discovery.master.resources.requests.cpu CPU request for master node feature discovery.
      ## @param azure-aks-gpu-operator.node-feature-discovery.master.resources.requests.memory Memory request for master node feature discovery.
      resources:
        requests:
          cpu: 10m
          memory: 200Mi
      ## @skip azure-aks-gpu-operator.node-feature-discovery.master.tolerations
      tolerations:
        - key: node-role.kubernetes.io/master
          operator: Equal
          effect: NoSchedule
        - key: node-role.kubernetes.io/control-plane
          operator: Equal
          effect: NoSchedule
        - key: kubernetes.azure.com/scalesetpriority
          operator: Equal
          value: spot
          effect: NoSchedule
    worker:
      ## @param azure-aks-gpu-operator.node-feature-discovery.worker.resources.requests.cpu CPU request for worker node feature discovery.
      ## @param azure-aks-gpu-operator.node-feature-discovery.worker.resources.requests.memory Memory request for worker node feature discovery.
      ## @param azure-aks-gpu-operator.node-feature-discovery.worker.resources.limits.cpu CPU limit for worker node feature discovery.
      ## @param azure-aks-gpu-operator.node-feature-discovery.worker.resources.limits.memory Memory limit for worker node feature discovery.
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
        limits:
          cpu: 50m
          memory: 300Mi
      affinity:
        ## @skip azure-aks-gpu-operator.node-feature-discovery.worker.affinity.nodeAffinity
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.azure.com/accelerator
                    operator: In
                    values:
                      - nvidia
      ## @skip azure-aks-gpu-operator.node-feature-discovery.worker.tolerations
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        - key: sku
          operator: Equal
          value: gpu
          effect: NoSchedule
        - key: kubernetes.azure.com/scalesetpriority
          operator: Equal
          value: spot
          effect: NoSchedule
    gc:
      ## @param azure-aks-gpu-operator.node-feature-discovery.gc.enable Enable node feature discovery garbage collector.
      enable: true
      ## @param azure-aks-gpu-operator.node-feature-discovery.gc.interval Interval between two garbage collection runs.
      interval: 30m
      ## @param azure-aks-gpu-operator.node-feature-discovery.gc.resources.requests.cpu CPU request for node feature discovery garbage collector.
      ## @param azure-aks-gpu-operator.node-feature-discovery.gc.resources.requests.memory Memory request for node feature discovery garbage collector.
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      affinity:
        ## @skip azure-aks-gpu-operator.node-feature-discovery.gc.affinity.nodeAffinity
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.azure.com/accelerator
                    operator: NotIn
                    values:
                      - nvidia
      ## @skip azure-aks-gpu-operator.node-feature-discovery.gc.tolerations
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        - key: kubernetes.azure.com/scalesetpriority
          operator: Equal
          value: spot
          effect: NoSchedule

  ## Daemonsets configuration
  daemonsets:
    ## @param azure-aks-gpu-operator.daemonsets.updateStrategy Update Strategy for Daemonsets - one of ["OnDelete", "RollingUpdate"]
    # This is set to OnDelete to protect against pod failures in case device plugin is unavailable during a kubelet restart (caused by toolkit container restart)
    # The downside being Daemonset will not be updated until it is manually deleted,
    # which is mostly okay for gpu case - effect of toolkit, device plugin daemonsets are limited to the node they run on.
    # If needed, older nodes can be drained to force a newer versions on a newer nodes
    # This will be changed to "RollingUpdate" when newer kubelet versions with allocation issue fix become the norm
    # We would also like to use `daemonsets.rollingUpdate.maxSurge: 1` which is not supported yet
    updateStrategy: OnDelete
    ## @param azure-aks-gpu-operator.daemonsets.priorityClassName Priority class for Daemonsets
    priorityClassName: system-node-critical
    ## @skip azure-aks-gpu-operator.daemonsets.tolerations
    tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      - key: sku
        operator: Equal
        value: gpu
        effect: NoSchedule
      - key: kubernetes.azure.com/scalesetpriority
        operator: Equal
        value: spot
        effect: NoSchedule

  ## Driver configuration
  driver:
    ## @param azure-aks-gpu-operator.driver.enabled Enable/Disable driver installation.
    enabled: false

  ## Toolkit configuration.
  toolkit:
    ## @param azure-aks-gpu-operator.toolkit.enabled Enable/Disable nvidia container toolkit installation.
    enabled: true
    ## @param azure-aks-gpu-operator.toolkit.version Version of the toolkit. Note for Aure Linux change `-ubuntu20.04` to `-ubi8`. However at the time of writing Azure Linux only supports V100 and T4 GPUs
    version: v1.17.2-ubuntu20.04
    ## @skip azure-aks-gpu-operator.toolkit.env
    env:
      - name: ACCEPT_NVIDIA_VISIBLE_DEVICES_ENVVAR_WHEN_UNPRIVILEGED
        value: 'false'
      - name: ACCEPT_NVIDIA_VISIBLE_DEVICES_AS_VOLUME_MOUNTS
        value: 'true'

  ## MIG Configuration
  mig:
    ## @param azure-aks-gpu-operator.mig.strategy migStrategy for mig node, single or mixed
    strategy: none
  
  ## Device Plugin configuration.
  devicePlugin:
    ## @param azure-aks-gpu-operator.devicePlugin.enabled Enable/Disable nvidia device plugin installation.
    enabled: true
    ## @skip azure-aks-gpu-operator.devicePlugin.env
    env:
      - name: PASS_DEVICE_SPECS
        value: 'true'
      - name: DEVICE_LIST_STRATEGY
        value: volume-mounts
      - name: DEVICE_ID_STRATEGY
        value: index
    ## @skip azure-aks-gpu-operator.devicePlugin.config
    config:
      data:
        all: ''
        time-sliced-10: |-
          version: v1
          sharing:
            timeSlicing:
              renameByDefault: true
              resources:
              - name: nvidia.com/gpu
                replicas: 10
      name: ''
      create: false
      default: ''

  ## DCGM configuration
  dcgm:
    ## @param azure-aks-gpu-operator.dcgm.enabled Enabled/Disable standalone DCGM.
    enabled: false
    ## @param azure-aks-gpu-operator.dcgm.version Image tag for DCGM container. Find all image tags at https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cloud-native/containers/dcgm/tags
    version: 3.3.8-1-ubuntu22.04
    ## @param azure-aks-gpu-operator.dcgm.resources.requests.cpu CPU request for standalone DCGM container
    ## @param azure-aks-gpu-operator.dcgm.resources.requests.memory Memory request for standalone DCGM container
    ## @param azure-aks-gpu-operator.dcgm.resources.limits.cpu CPU limit for standalone DCGM container
    ## @param azure-aks-gpu-operator.dcgm.resources.limits.memory Memory limit for standalone DCGM container
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 100m
        memory: 1000Mi

  ## DCGM Exporter configuration.
  dcgmExporter:
    ## @param azure-aks-gpu-operator.dcgmExporter.enabled Enabled/Disable DCGM Exporter.
    enabled: true
    ## @param azure-aks-gpu-operator.dcgmExporter.version Image tag version for DCGM Exporter. Find all tags at https://catalog.ngc.nvidia.com/orgs/nvidia/teams/k8s/containers/dcgm-exporter/tags
    version: 3.3.8-3.6.0-ubuntu22.04
    serviceMonitor:
      ## @param azure-aks-gpu-operator.dcgmExporter.serviceMonitor.enabled Enable or disable ServiceMonitor for DCGM Exporter.
      enabled: false
      ## @skip azure-aks-gpu-operator.dcgmExporter.serviceMonitor.honorLabels
      honorLabels: true
      ## @skip azure-aks-gpu-operator.dcgmExporter.serviceMonitor.additionalLabels
      additionalLabels:
        release: prometheus
      ## @skip azure-aks-gpu-operator.dcgmExporter.serviceMonitor.relabelings
      relabelings:
        - action: replace
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: kubernetes_node
        - action: replace
          targetLabel: job
          replacement: gpu-metrics
    ## @skip azure-aks-gpu-operator.dcgmExporter.env
    env:
      - name: DCGM_EXPORTER_KUBERNETES_GPU_ID_TYPE
        value: uid
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_COLLECTORS
        value: /etc/dcgm-exporter/dcp-metrics-included.csv
    ## @param azure-aks-gpu-operator.dcgmExporter.resources.requests.cpu CPU request for the DCGM Exporter.
    ## @param azure-aks-gpu-operator.dcgmExporter.resources.requests.memory Memory request for the DCGM Exporter.
    ## @param azure-aks-gpu-operator.dcgmExporter.resources.limits.cpu CPU limit for the DCGM Exporter.
    ## @param azure-aks-gpu-operator.dcgmExporter.resources.limits.memory Memory limit for the DCGM Exporter.
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 100m
        memory: 1000Mi
    ## @param azure-aks-gpu-operator.dcgmExporter.args Arguments for the DCGM Exporter.
    args: ["-c", "5000"]

  ## MIG Manager configuration.
  migManager:
    ## @skip azure-aks-gpu-operator.migManager.enabled
    enabled: false

  ## Validator configuration.
  validator:
    cuda:
      ## @skip azure-aks-gpu-operator.validator.cuda.env
      env:
        - name: WITH_WORKLOAD
          value: "false"
    plugin:
      ## @skip azure-aks-gpu-operator.validator.plugin.env
      env:
        - name: WITH_WORKLOAD
          value: "false"

## @section civo-talos-gpu-operator Configuration for the Civo Talos GPU Operator. This section will only be used when clusterType.civoTalos is set to true.
##
civo-talos-gpu-operator:
  ## Node Feature Discovery enabled/disable configuration.
  nfd:
    ## @param civo-talos-gpu-operator.nfd.enabled Enable/Disable node feature discovery.
    enabled: true
  
  ## GPU Feature Discovery configuration.
  gfd:
    ## @param civo-talos-gpu-operator.gfd.enabled Enable/Disable gpu feature discovery.
    enabled: true
  
  ## Operator configuration.
  operator:
    ## @param civo-talos-gpu-operator.operator.upgradeCRD upgrade CRD on chart upgrade
    upgradeCRD: true
    ## @param civo-talos-gpu-operator.operator.cleanupCRD cleanup CRD on chart uninstall
    cleanupCRD: true
    ## Resource configuration for operator requests and limits.
    ## @param civo-talos-gpu-operator.operator.resources.requests.cpu CPU request for the operator.
    ## @param civo-talos-gpu-operator.operator.resources.requests.memory Memory request for the operator.
    ## @param civo-talos-gpu-operator.operator.resources.limits.cpu CPU limit for the operator.
    ## @param civo-talos-gpu-operator.operator.resources.limits.memory Memory limit for the operator.
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 50m
        memory: 300Mi
  
  ## Node Feature Discovery configuration.
  node-feature-discovery:
    ## @param civo-talos-gpu-operator.node-feature-discovery.enableNodeFeatureApi Enable/Disable node feature api in node-feature-discovery.
    enableNodeFeatureApi: true
    master:
      ## @param civo-talos-gpu-operator.node-feature-discovery.master.resources.requests.cpu CPU request for master node feature discovery.
      ## @param civo-talos-gpu-operator.node-feature-discovery.master.resources.requests.memory Memory request for master node feature discovery.
      resources:
        requests:
          cpu: 10m
          memory: 200Mi
    worker:
      ## @param civo-talos-gpu-operator.node-feature-discovery.worker.resources.requests.cpu CPU request for worker node feature discovery.
      ## @param civo-talos-gpu-operator.node-feature-discovery.worker.resources.requests.memory Memory request for worker node feature discovery.
      ## @param civo-talos-gpu-operator.node-feature-discovery.worker.resources.limits.cpu CPU limit for worker node feature discovery.
      ## @param civo-talos-gpu-operator.node-feature-discovery.worker.resources.limits.memory Memory limit for worker node feature discovery.
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
        limits:
          cpu: 50m
          memory: 300Mi
      affinity:
        ## @skip civo-talos-gpu-operator.node-feature-discovery.worker.affinity.nodeAffinity
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node.kubernetes.io/instance-type
                    operator: In
                    values:
                      - g4g.40.kube.small
                      - g4g.40.kube.medium
                      - g4g.40.kube.large
                      - g4g.40.kube.xlarge
                      - g4g.kube.small
                      - g4g.kube.medium
                      - g4g.kube.large
                      - g4g.kube.xlarge
                      - an.g1.l40s.kube.x1
                      - an.g1.l40s.kube.x2
                      - an.g1.l40s.kube.x4
                      - an.g1.l40s.kube.x8
      tolerations:
        ## @skip civo-talos-gpu-operator.node-feature-discovery.worker.tolerations[0]
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
    gc:
      ## @param civo-talos-gpu-operator.node-feature-discovery.gc.enable Enable node feature discovery garbage collector.
      enable: true
      ## @param civo-talos-gpu-operator.node-feature-discovery.gc.interval Interval between two garbage collection runs.
      interval: 30m
      ## @param civo-talos-gpu-operator.node-feature-discovery.gc.resources.requests.cpu CPU request for node feature discovery garbage collector.
      ## @param civo-talos-gpu-operator.node-feature-discovery.gc.resources.requests.memory Memory request for node feature discovery garbage collector.
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      affinity:
        ## @skip civo-talos-gpu-operator.node-feature-discovery.gc.affinity.nodeAffinity
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node.kubernetes.io/instance-type
                    operator: NotIn
                    values:
                      - g4g.40.kube.small
                      - g4g.40.kube.medium
                      - g4g.40.kube.large
                      - g4g.40.kube.xlarge
                      - g4g.kube.small
                      - g4g.kube.medium
                      - g4g.kube.large
                      - g4g.kube.xlarge
                      - an.g1.l40s.kube.x1
                      - an.g1.l40s.kube.x2
                      - an.g1.l40s.kube.x4
                      - an.g1.l40s.kube.x8
      tolerations:
        ## @skip civo-talos-gpu-operator.node-feature-discovery.gc.tolerations[0]
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
    
  ## Driver configuration
  driver:
    ## @param civo-talos-gpu-operator.driver.enabled Enable/Disable driver installation.
    enabled: false
  
  ## Toolkit configuration.
  toolkit:
    ## @param civo-talos-gpu-operator.toolkit.enabled Enable/Disable nvidia container toolkit installation.
    enabled: false
  
  ## Device Plugin configuration.
  devicePlugin:
    ## @param civo-talos-gpu-operator.devicePlugin.enabled Enable/Disable nvidia device plugin installation.
    enabled: true
    ## @skip civo-talos-gpu-operator.devicePlugin.env
    env:
      - name: PASS_DEVICE_SPECS
        value: 'true'
      - name: DEVICE_LIST_STRATEGY
        value: volume-mounts
      - name: DEVICE_ID_STRATEGY
        value: index
    ## @skip civo-talos-gpu-operator.devicePlugin.config
    config:
      data:
        all: ''
        time-sliced-10: |-
          version: v1
          sharing:
            timeSlicing:
              renameByDefault: true
              resources:
              - name: nvidia.com/gpu
                replicas: 10
      name: ''
      create: false
      default: ''

  ## DCGM configuration
  dcgm:
    ## @param civo-talos-gpu-operator.dcgm.enabled Enabled/Disable standalone DCGM.
    enabled: true
    ## @param civo-talos-gpu-operator.dcgm.version Image tag for DCGM container. Find all image tags at https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cloud-native/containers/dcgm/tags
    version: 3.3.8-1-ubuntu22.04
    ## @param civo-talos-gpu-operator.dcgm.resources.requests.cpu CPU request for standalone DCGM container
    ## @param civo-talos-gpu-operator.dcgm.resources.requests.memory Memory request for standalone DCGM container
    ## @param civo-talos-gpu-operator.dcgm.resources.limits.cpu CPU limit for standalone DCGM container
    ## @param civo-talos-gpu-operator.dcgm.resources.limits.memory Memory limit for standalone DCGM container
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 50m
        memory: 400Mi

  ## DCGM Exporter configuration.
  dcgmExporter:
    ## @param civo-talos-gpu-operator.dcgmExporter.enabled Enabled/Disable DCGM Exporter.
    enabled: true
    ## @param civo-talos-gpu-operator.dcgmExporter.version Image tag version for DCGM Exporter. Find all tags at https://catalog.ngc.nvidia.com/orgs/nvidia/teams/k8s/containers/dcgm-exporter/tags
    version: 3.3.8-3.6.0-ubuntu22.04
    serviceMonitor:
      ## @param civo-talos-gpu-operator.dcgmExporter.serviceMonitor.enabled Enable or disable ServiceMonitor for DCGM Exporter.
      enabled: false
      ## @skip civo-talos-gpu-operator.dcgmExporter.serviceMonitor.honorLabels
      honorLabels: true
      ## @skip civo-talos-gpu-operator.dcgmExporter.serviceMonitor.additionalLabels
      additionalLabels:
        release: prometheus
      ## @skip civo-talos-gpu-operator.dcgmExporter.serviceMonitor.relabelings
      relabelings:
        - action: replace
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: kubernetes_node
        - action: replace
          targetLabel: job
          replacement: gpu-metrics
    ## @skip civo-talos-gpu-operator.dcgmExporter.env
    env:
      - name: DCGM_EXPORTER_KUBERNETES_GPU_ID_TYPE
        value: uid
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_COLLECTORS
        value: /etc/dcgm-exporter/dcp-metrics-included.csv
    ## @param civo-talos-gpu-operator.dcgmExporter.resources.requests.cpu CPU request for the DCGM Exporter.
    ## @param civo-talos-gpu-operator.dcgmExporter.resources.requests.memory Memory request for the DCGM Exporter.
    ## @param civo-talos-gpu-operator.dcgmExporter.resources.limits.cpu CPU limit for the DCGM Exporter.
    ## @param civo-talos-gpu-operator.dcgmExporter.resources.limits.memory Memory limit for the DCGM Exporter.
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 100m
        memory: 1000Mi
    ## @param civo-talos-gpu-operator.dcgmExporter.args Arguments for the DCGM Exporter.
    args: ["-c", "5000"]

  ## MIG Configuration
  mig:
    ## @param civo-talos-gpu-operator.mig.strategy migStrategy for mig node, single or mixed
    strategy: none

  ## MIG Manager configuration.
  migManager:
    ## @skip civo-talos-gpu-operator.migManager.enabled
    enabled: false

  ## Validator configuration.
  validator:
    cuda:
      ## @skip civo-talos-gpu-operator.validator.cuda.env
      env:
        - name: WITH_WORKLOAD
          value: "false"
    plugin:
      ## @skip civo-talos-gpu-operator.validator.plugin.env
      env:
        - name: WITH_WORKLOAD
          value: "false"

## @section generic-gpu-operator Configuration for the GPU Operator. This section will only be used when clusterType.generic is set to true.
generic-gpu-operator:
  ## Node Feature Discovery enabled/disable configuration.
  nfd:
    ## @param generic-gpu-operator.nfd.enabled Enable/Disable node feature discovery.
    enabled: true
  
  ## GPU Feature Discovery configuration.
  gfd:
    ## @param generic-gpu-operator.gfd.enabled Enable/Disable gpu feature discovery.
    enabled: true
    ## @skip generic-gpu-operator.gfd.env
    env:
      - name: GFD_SLEEP_INTERVAL
        value: 60s
      - name: GFD_FAIL_ON_INIT_ERROR
        value: "true"
  
  ## Operator configuration.
  operator:
    ## @param generic-gpu-operator.operator.upgradeCRD upgrade CRD on chart upgrade
    upgradeCRD: true
    ## @param generic-gpu-operator.operator.cleanupCRD cleanup CRD on chart uninstall
    cleanupCRD: true
    ## Resource configuration for operator requests and limits.
    ## @param generic-gpu-operator.operator.resources.requests.cpu CPU request for the operator.
    ## @param generic-gpu-operator.operator.resources.requests.memory Memory request for the operator.
    ## @param generic-gpu-operator.operator.resources.limits.cpu CPU limit for the operator.
    ## @param generic-gpu-operator.operator.resources.limits.memory Memory limit for the operator.
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 50m
        memory: 300Mi
    ## @skip generic-gpu-operator.operator.tolerations
    tolerations:
      - key: node-role.kubernetes.io/master
        operator: Equal
        value: ''
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Equal
        value: ''
        effect: NoSchedule
  
  ## Node Feature Discovery configuration.
  node-feature-discovery:
    ## @param generic-gpu-operator.node-feature-discovery.enableNodeFeatureApi Enable/Disable node feature api in node-feature-discovery.
    enableNodeFeatureApi: true
    master:
      ## @param generic-gpu-operator.node-feature-discovery.master.resources.requests.cpu CPU request for master node feature discovery.
      ## @param generic-gpu-operator.node-feature-discovery.master.resources.requests.memory Memory request for master node feature discovery.
      resources:
        requests:
          cpu: 10m
          memory: 200Mi
      ## @skip generic-gpu-operator.node-feature-discovery.master.tolerations
      tolerations:
        - key: node-role.kubernetes.io/master
          operator: Equal
          effect: NoSchedule
        - key: node-role.kubernetes.io/control-plane
          operator: Equal
          effect: NoSchedule
    worker:
      ## @param generic-gpu-operator.node-feature-discovery.worker.resources.requests.cpu CPU request for worker node feature discovery.
      ## @param generic-gpu-operator.node-feature-discovery.worker.resources.requests.memory Memory request for worker node feature discovery.
      ## @param generic-gpu-operator.node-feature-discovery.worker.resources.limits.cpu CPU limit for worker node feature discovery.
      ## @param generic-gpu-operator.node-feature-discovery.worker.resources.limits.memory Memory limit for worker node feature discovery.
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
        limits:
          cpu: 50m
          memory: 300Mi
      ## @skip generic-gpu-operator.node-feature-discovery.worker.tolerations
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        - key: sku
          operator: Equal
          value: gpu
          effect: NoSchedule
    gc:
      ## @param generic-gpu-operator.node-feature-discovery.gc.enable Enable node feature discovery garbage collector.
      enable: true
      ## @param generic-gpu-operator.node-feature-discovery.gc.interval Interval between two garbage collection runs.
      interval: 30m
      ## @param generic-gpu-operator.node-feature-discovery.gc.resources.requests.cpu CPU request for node feature discovery garbage collector.
      ## @param generic-gpu-operator.node-feature-discovery.gc.resources.requests.memory Memory request for node feature discovery garbage collector.
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      ## @skip generic-gpu-operator.node-feature-discovery.gc.tolerations
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

  ## Daemonsets configuration
  daemonsets:
    ## @param generic-gpu-operator.daemonsets.updateStrategy Update Strategy for Daemonsets - one of ["OnDelete", "RollingUpdate"]
    # This is set to OnDelete to protect against pod failures in case device plugin is unavailable during a kubelet restart (caused by toolkit container restart)
    # The downside being Daemonset will not be updated until it is manually deleted,
    # which is mostly okay for gpu case - effect of toolkit, device plugin daemonsets are limited to the node they run on.
    # If needed, older nodes can be drained to force a newer versions on a newer nodes
    # This will be changed to "RollingUpdate" when newer kubelet versions with allocation issue fix become the norm
    # We would also like to use `daemonsets.rollingUpdate.maxSurge: 1` which is not supported yet
    updateStrategy: OnDelete
    ## @param generic-gpu-operator.daemonsets.priorityClassName Priority class for Daemonsets
    priorityClassName: system-node-critical
    ## @skip generic-gpu-operator.daemonsets.tolerations
    tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      - key: sku
        operator: Equal
        value: gpu
        effect: NoSchedule

  ## Driver configuration
  driver:
    ## @param generic-gpu-operator.driver.enabled Enable/Disable driver installation.
    enabled: true

  ## Toolkit configuration.
  toolkit:
    ## @param generic-gpu-operator.toolkit.enabled Enable/Disable nvidia container toolkit installation.
    enabled: true
    ## @param generic-gpu-operator.toolkit.version Version of the toolkit. 
    version: v1.17.2-ubuntu20.04
    ## @skip generic-gpu-operator.toolkit.env
    env:
      - name: ACCEPT_NVIDIA_VISIBLE_DEVICES_ENVVAR_WHEN_UNPRIVILEGED
        value: 'false'
      - name: ACCEPT_NVIDIA_VISIBLE_DEVICES_AS_VOLUME_MOUNTS
        value: 'true'

  ## Device Plugin configuration.
  devicePlugin:
    ## @param generic-gpu-operator.devicePlugin.enabled Enable/Disable nvidia device plugin installation.
    enabled: true
    ## @skip generic-gpu-operator.devicePlugin.env
    env:
      - name: PASS_DEVICE_SPECS
        value: 'true'
      - name: DEVICE_LIST_STRATEGY
        value: volume-mounts
      - name: DEVICE_ID_STRATEGY
        value: index
    ## @skip generic-gpu-operator.devicePlugin.config
    config:
      data:
        all: ''
        time-sliced-10: |-
          version: v1
          sharing:
            timeSlicing:
              renameByDefault: true
              resources:
              - name: nvidia.com/gpu
                replicas: 10
      name: ''
      create: false
      default: ''

  ## DCGM configuration
  dcgm:
    ## @param generic-gpu-operator.dcgm.enabled Enabled/Disable standalone DCGM.
    enabled: false
    ## @param generic-gpu-operator.dcgm.version Image tag for DCGM container. Find all image tags at https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cloud-native/containers/dcgm/tags
    version: 3.3.8-1-ubuntu22.04
    ## @param generic-gpu-operator.dcgm.resources.requests.cpu CPU request for standalone DCGM container
    ## @param generic-gpu-operator.dcgm.resources.requests.memory Memory request for standalone DCGM container
    ## @param generic-gpu-operator.dcgm.resources.limits.cpu CPU limit for standalone DCGM container
    ## @param generic-gpu-operator.dcgm.resources.limits.memory Memory limit for standalone DCGM container
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 100m
        memory: 1000Mi

  ## DCGM Exporter configuration.
  dcgmExporter:
    ## @param generic-gpu-operator.dcgmExporter.enabled Enabled/Disable DCGM Exporter.
    enabled: true
    ## @param generic-gpu-operator.dcgmExporter.version Image tag version for DCGM Exporter. Find all tags at https://catalog.ngc.nvidia.com/orgs/nvidia/teams/k8s/containers/dcgm-exporter/tags
    version: 3.3.8-3.6.0-ubuntu22.04
    serviceMonitor:
      ## @param generic-gpu-operator.dcgmExporter.serviceMonitor.enabled Enable or disable ServiceMonitor for DCGM Exporter.
      enabled: false
      ## @skip generic-gpu-operator.dcgmExporter.serviceMonitor.honorLabels
      honorLabels: true
      ## @skip generic-gpu-operator.dcgmExporter.serviceMonitor.additionalLabels
      additionalLabels:
        release: prometheus
      ## @skip generic-gpu-operator.dcgmExporter.serviceMonitor.relabelings
      relabelings:
        - action: replace
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: kubernetes_node
        - action: replace
          targetLabel: job
          replacement: gpu-metrics
    ## @skip generic-gpu-operator.dcgmExporter.env
    env:
      - name: DCGM_EXPORTER_KUBERNETES_GPU_ID_TYPE
        value: uid
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_COLLECTORS
        value: /etc/dcgm-exporter/dcp-metrics-included.csv
    ## @param generic-gpu-operator.dcgmExporter.resources.requests.cpu CPU request for the DCGM Exporter.
    ## @param generic-gpu-operator.dcgmExporter.resources.requests.memory Memory request for the DCGM Exporter.
    ## @param generic-gpu-operator.dcgmExporter.resources.limits.cpu CPU limit for the DCGM Exporter.
    ## @param generic-gpu-operator.dcgmExporter.resources.limits.memory Memory limit for the DCGM Exporter.
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 100m
        memory: 1000Mi
    ## @param generic-gpu-operator.dcgmExporter.args Arguments for the DCGM Exporter.
    args: ["-c", "5000"]

  ## MIG Configuration
  mig:
    ## @param generic-gpu-operator.mig.strategy migStrategy for mig node, single or mixed
    strategy: none

  ## MIG Manager configuration.
  migManager:
    ## @skip generic-gpu-operator.migManager.enabled
    enabled: true

  ## Validator configuration.
  validator:
    cuda:
      ## @skip generic-gpu-operator.validator.cuda.env
      env:
        - name: WITH_WORKLOAD
          value: "false"
    plugin:
      ## @skip generic-gpu-operator.validator.plugin.env
      env:
        - name: WITH_WORKLOAD
          value: "false"
