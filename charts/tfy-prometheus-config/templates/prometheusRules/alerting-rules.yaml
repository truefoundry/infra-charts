{{- if .Values.prometheusRules.containerRules.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ .Values.prometheusRules.containerRules.name }}
  labels:
    {{- include "containerRule.labels" . | nindent 4 }}
spec:
  groups:
  - name: Alerting
    rules:
    - for: 5m
      expr: >-
        sum(increase(container_cpu_cfs_throttled_periods_total{job="kubelet", container!=""}[5m]))
        by (pod, container, namespace) /
        sum(increase(container_cpu_cfs_periods_total{job="kubelet"}[5m])) by
        (pod, container, namespace) > ( 25 / 100 )
      alert: ContainerHighThrottleRate
      labels:
        entity: container
        severity: warning
      annotations:
        summary: Container high throttle rate (instance {{ `{{ $labels.instance }}` }})
        description: |-
          Container is being throttled
            VALUE = {{`{{ $value }}`}}
    - for: 15m
      expr: >-
        sum by (namespace, pod)
        (kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown|Failed"}) > 0
      alert: KubernetesPodNotHealthy
      labels:
        entity: pod
        severity: critical
      annotations:
        summary: >-
          Kubernetes Pod not healthy ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}})
        description: >-
          Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been in a
          non-running state for longer than 15 minutes.
            VALUE = {{`{{ $value }}`}}
    - for: 2m
      expr: increase(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[1m]) > 3
      alert: KubernetesPodCrashLooping
      labels:
        entity: pod
        severity: warning
      annotations:
        summary: >-
          Kubernetes pod crash looping ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}})
        description: |-
          Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} is crash looping
            VALUE = {{`{{ $value }}`}}
    - for: 2m
      expr: >-
        sum(increase(container_oom_events_total{job="kubelet", container!=""}[5m]))
        by (container, pod, namespace) > 0
      alert: ContainerOOMKilled
      labels:
        entity: container
        severity: critical
      annotations:
        summary: >-
          Container OOM Killed ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}}/{{`{{ $labels.container }}`}})
        description: |-
          Container {{`{{ $labels.container }}`}} in pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}}
          has been OOM killed in the last 5 minutes
            VALUE = {{`{{ $value }}`}} times
    - for: 10m
      expr: >-
        (sum(kubelet_volume_stats_used_bytes{job="kubelet"}) by (persistentvolumeclaim, namespace) /
        sum(kubelet_volume_stats_capacity_bytes{job="kubelet"}) by (persistentvolumeclaim, namespace)) * 100 > 90
      alert: PersistentVolumeUsageHigh
      labels:
        entity: persistentVolume
        severity: critical
      annotations:
        summary: >-
          Persistent Volume usage is above 90% ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.persistentvolumeclaim }}`}})
        description: |-
          Persistent Volume {{`{{ $labels.persistentvolumeclaim }}`}} in namespace {{`{{ $labels.namespace }}`}}
          is using more than 90% of its capacity.
            VALUE = {{`{{ $value }}`}}%
    - for: 5m
      expr: (1 - (node_filesystem_avail_bytes{job="node-exporter"} / node_filesystem_size_bytes{job="node-exporter"})) > 0.9
      alert: NodeDiskPressure
      labels:
        entity: node
        severity: critical
      annotations:
        summary: >-
          Node disk usage is above 90% ({{`{{ $labels.instance }}`}})
        description: |-
          Node {{`{{ $labels.instance }}`}} is experiencing disk pressure.
            VALUE = {{`{{ $value }}`}}%
    - for: 5m
      expr: node_memory_MemAvailable_bytes{job="node-exporter"} / node_memory_MemTotal_bytes{job="node-exporter"} < 0.1
      alert: NodeMemoryPressure
      labels:
        entity: node
        severity: critical
      annotations:
        summary: >-
          Node memory available is below 10% ({{`{{ $labels.instance }}`}})
        description: |-
          Node {{`{{ $labels.instance }}`}} is experiencing memory pressure.
            VALUE = {{`{{ $value }}`}}%
    - for: 5m
      expr: rate(node_cpu_seconds_total{job="node-exporter", mode="idle"}[5m]) < 0.1
      alert: NodeCPUPressure
      labels:
        entity: node
        severity: critical
      annotations:
        summary: >-
          Node CPU usage is above 90% ({{`{{ $labels.instance }}`}})
        description: |-
          Node {{`{{ $labels.instance }}`}} is experiencing CPU pressure.
            VALUE = {{`{{ $value }}`}}%
    - for: 5m
      expr: kube_node_status_condition{job="kube-state-metrics", condition="Ready", status="true"} == 0
      alert: NodeNotReady
      labels:
        entity: node
        severity: critical
      annotations:
        summary: >-
          Node is not ready ({{`{{ $labels.node }}`}})
        description: |-
          Node {{`{{ $labels.node }}`}} is not in a ready state.
            VALUE = {{`{{ $value }}`}}
    - for: 5m
      expr: kube_node_status_condition{job="kube-state-metrics", condition="NetworkUnavailable", status="true"} == 1
      alert: NodeNetworkUnavailable
      labels:
        entity: node
        severity: critical
      annotations:
        summary: >-
          Node network is unavailable ({{`{{ $labels.node }}`}})
        description: |-
          Node {{`{{ $labels.node }}`}} network is unavailable.
            VALUE = {{`{{ $value }}`}}
{{- end }}