altinity-clickhouse-operator:
  operator:
    image:
      # operator.image.repository -- image repository
      repository: altinity/clickhouse-operator
      # operator.image.tag -- image tag (chart's appVersion value will be used if not set)
      tag: ""
      # operator.image.pullPolicy -- image pull policy
      pullPolicy: IfNotPresent
    containerSecurityContext: {}
    # operator.resources -- custom resource configuration, look `kubectl explain pod.spec.containers.resources` for details
    resources: {}
    #  limits:
    #    cpu: 100m
    #    memory: 128Mi
    #  requests:
    #    cpu: 100m
    #    memory: 128Mi

    # operator.env -- additional environment variables for the clickhouse-operator container in deployment
    # possible format value [{"name": "SAMPLE", "value": "text"}]
    env: []
  metrics:
    enabled: true
    image:
      # metrics.image.repository -- image repository
      repository: altinity/metrics-exporter
      # metrics.image.tag -- image tag (chart's appVersion value will be used if not set)
      tag: ""
      # metrics.image.pullPolicy -- image pull policy
      pullPolicy: IfNotPresent
    containerSecurityContext: {}
    # metrics.resources -- custom resource configuration
    resources: {}
    #  limits:
    #    cpu: 100m
    #    memory: 128Mi
    #  requests:
    #    cpu: 100m
    #    memory: 128Mi

    # metrics.env -- additional environment variables for the deployment of metrics-exporter containers
    # possible format value [{"name": "SAMPLE", "value": "text"}]
    env: []
  # imagePullSecrets -- image pull secret for private images in clickhouse-operator pod
  #  possible value format [{"name":"your-secret-name"}]
  #  look `kubectl explain pod.spec.imagePullSecrets` for details
  imagePullSecrets: []
  # podAnnotations -- annotations to add to the clickhouse-operator pod, look `kubectl explain pod.spec.annotations` for details
  podAnnotations:
    prometheus.io/port: '8888'
    prometheus.io/scrape: 'true'
    clickhouse-operator-metrics/port: '9999'
    clickhouse-operator-metrics/scrape: 'true'
  # nameOverride -- override name of the chart
  nameOverride: ""
  # fullnameOverride -- full name of the chart.
  fullnameOverride: ""
  serviceAccount:
    # serviceAccount.create -- specifies whether a service account should be created
    create: true
    # serviceAccount.annotations -- annotations to add to the service account
    annotations: {}
    # serviceAccount.name -- the name of the service account to use; if not set and create is true, a name is generated using the fullname template
    name:
  rbac:
    # rbac.create -- specifies whether cluster roles and cluster role bindings should be created
    create: true
  secret:
    # secret.create -- create a secret with operator credentials
    create: true
    # secret.username -- operator credentials username
    username: clickhouse_operator
    # secret.password -- operator credentials password
    password: clickhouse_operator_password
  # nodeSelector -- node for scheduler pod assignment, look `kubectl explain pod.spec.nodeSelector` for details
  nodeSelector: {}
  # tolerations -- tolerations for scheduler pod assignment, look `kubectl explain pod.spec.tolerations` for details
  tolerations: []
  # affinity -- affinity for scheduler pod assignment, look `kubectl explain pod.spec.affinity` for details
  affinity: {}
  # podSecurityContext - operator deployment SecurityContext, look `kubectl explain pod.spec.securityContext` for details
  podSecurityContext: {}
  serviceMonitor:
    # serviceMonitor.enabled -- ServiceMonitor Custom resource is created for a (prometheus-operator)[https://github.com/prometheus-operator/prometheus-operator]
    enabled: false
    # serviceMonitor.additionalLabels -- additional labels for service monitor
    additionalLabels: {}
  # configs -- clickhouse-operator configs
  # @default -- check the values.yaml file for the config content, auto-generated from latest operator release
  configs:
    confdFiles: null
    configdFiles:
      01-clickhouse-01-listen.xml: |
        <!-- IMPORTANT -->
        <!-- This file is auto-generated -->
        <!-- Do not edit this file - all changes would be lost -->
        <!-- Edit appropriate template in the following folder: -->
        <!-- deploy/builder/templates-config -->
        <!-- IMPORTANT -->
        <yandex>
            <!-- Listen wildcard address to allow accepting connections from other containers and host network. -->
            <listen_host>::</listen_host>
            <listen_host>0.0.0.0</listen_host>
            <listen_try>1</listen_try>
        </yandex>
      01-clickhouse-02-logger.xml: |
        <!-- IMPORTANT -->
        <!-- This file is auto-generated -->
        <!-- Do not edit this file - all changes would be lost -->
        <!-- Edit appropriate template in the following folder: -->
        <!-- deploy/builder/templates-config -->
        <!-- IMPORTANT -->
        <yandex>
            <logger>
                <!-- Possible levels: https://github.com/pocoproject/poco/blob/devel/Foundation/include/Poco/Logger.h#L439 -->
                <level>debug</level>
                <log>/var/log/clickhouse-server/clickhouse-server.log</log>
                <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
                <size>1000M</size>
                <count>10</count>
                <!-- Default behavior is autodetection (log to console if not daemon mode and is tty) -->
                <console>1</console>
            </logger>
        </yandex>
      01-clickhouse-03-query_log.xml: |
        <!-- IMPORTANT -->
        <!-- This file is auto-generated -->
        <!-- Do not edit this file - all changes would be lost -->
        <!-- Edit appropriate template in the following folder: -->
        <!-- deploy/builder/templates-config -->
        <!-- IMPORTANT -->
        <yandex>
            <query_log replace="1">
                <database>system</database>
                <table>query_log</table>
                <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
                <flush_interval_milliseconds>7500</flush_interval_milliseconds>
            </query_log>
            <query_thread_log remove="1"/>
        </yandex>
      01-clickhouse-04-part_log.xml: |
        <!-- IMPORTANT -->
        <!-- This file is auto-generated -->
        <!-- Do not edit this file - all changes would be lost -->
        <!-- Edit appropriate template in the following folder: -->
        <!-- deploy/builder/templates-config -->
        <!-- IMPORTANT -->
        <yandex>
            <part_log replace="1">
                <database>system</database>
                <table>part_log</table>
                <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
                <flush_interval_milliseconds>7500</flush_interval_milliseconds>
            </part_log>
        </yandex>
      01-clickhouse-05-trace_log.xml: |-
        <!-- IMPORTANT -->
        <!-- This file is auto-generated -->
        <!-- Do not edit this file - all changes would be lost -->
        <!-- Edit appropriate template in the following folder: -->
        <!-- deploy/builder/templates-config -->
        <!-- IMPORTANT -->
        <yandex>
            <trace_log replace="1">
                <database>system</database>
                <table>trace_log</table>
                <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
                <flush_interval_milliseconds>7500</flush_interval_milliseconds>
            </trace_log>
        </yandex>
    files:
      config.yaml:
        # IMPORTANT
        # This file is auto-generated
        # Do not edit this file - all changes would be lost
        # Edit appropriate template in the following folder:
        # deploy/builder/templates-config
        # IMPORTANT
        #
        # Template parameters available:
        #   WATCH_NAMESPACES=
        #   CH_USERNAME_PLAIN=
        #   CH_PASSWORD_PLAIN=
        #   CH_CREDENTIALS_SECRET_NAMESPACE=
        #   CH_CREDENTIALS_SECRET_NAME=clickhouse-operator
        #   VERBOSITY=1

        ################################################
        ##
        ## Watch section
        ##
        ################################################
        watch:
          # List of namespaces where clickhouse-operator watches for events.
          # Concurrently running operators should watch on different namespaces.
          # IMPORTANT
          # Regexp is applicable.
          #namespaces: ["dev", "test"]
          namespaces: []
        clickhouse:
          configuration:
            ################################################
            ##
            ## Configuration files section
            ##
            ################################################
            file:
              # Each 'path' can be either absolute or relative.
              # In case path is absolute - it is used as is
              # In case path is relative - it is relative to the folder where configuration file you are reading right now is located.
              path:
                # Path to the folder where ClickHouse configuration files common for all instances within a CHI are located.
                common: config.d
                # Path to the folder where ClickHouse configuration files unique for each instance (host) within a CHI are located.
                host: conf.d
                # Path to the folder where ClickHouse configuration files with users' settings are located.
                # Files are common for all instances within a CHI.
                user: users.d
            ################################################
            ##
            ## Configuration users section
            ##
            ################################################
            user:
              # Default settings for user accounts, created by the operator.
              # IMPORTANT. These are not access credentials or settings for 'default' user account,
              # it is a template for filling out missing fields for all user accounts to be created by the operator,
              # with the following EXCEPTIONS:
              # 1. 'default' user account DOES NOT use provided password, but uses all the rest of the fields.
              #    Password for 'default' user account has to be provided explicitly, if to be used.
              # 2. CHOP user account DOES NOT use:
              #    - profile setting. It uses predefined profile called 'clickhouse_operator'
              #    - quota setting. It uses empty quota name.
              #    - networks IP setting. Operator specifies 'networks/ip' user setting to match operators' pod IP only.
              #    - password setting. Password for CHOP account is used from 'clickhouse.access.*' section
              default:
                # Default values for ClickHouse user account(s) created by the operator
                #   1. user/profile - string
                #   2. user/quota - string
                #   3. user/networks/ip - multiple strings
                #   4. user/password - string
                # These values can be overwritten on per-user basis.
                profile: "default"
                quota: "default"
                networksIP:
                  - "::1"
                  - "127.0.0.1"
                password: "default"
            ################################################
            ##
            ## Configuration network section
            ##
            ################################################
            network:
              # Default host_regexp to limit network connectivity from outside
              hostRegexpTemplate: "(chi-{chi}-[^.]+\\d+-\\d+|clickhouse\\-{chi})\\.{namespace}\\.svc\\.cluster\\.local$"
          ################################################
          ##
          ## Configuration restart policy section
          ## Configuration restart policy describes what configuration changes require ClickHouse restart
          ##
          ################################################
          configurationRestartPolicy:
            rules:
              # IMPORTANT!
              # Special version of "*" - default version - has to satisfy all ClickHouse versions.
              # Default version will also be used in case ClickHouse version is unknown.
              # ClickHouse version may be unknown due to host being down - for example, because of incorrect "settings" section.
              # ClickHouse is not willing to start in case incorrect/unknown settings are provided in config file.
              - version: "*"
                rules:
                  - settings/*: "yes"
                  - settings/dictionaries_config: "no"
                  - settings/logger: "no"
                  - settings/macros/*: "no"
                  - settings/max_server_memory_*: "no"
                  - settings/max_*_to_drop: "no"
                  - settings/max_concurrent_queries: "no"
                  - settings/models_config: "no"
                  - settings/user_defined_executable_functions_config: "no"
                  - zookeeper/*: "yes"
                  - files/*.xml: "yes"
                  - files/config.d/*.xml: "yes"
                  - files/config.d/*dict*.xml: "no"
                  - profiles/default/background_*_pool_size: "yes"
                  - profiles/default/max_*_for_server: "yes"
              - version: "21.*"
                rules:
                  - settings/logger: "yes"
          #################################################
          ##
          ## Access to ClickHouse instances
          ##
          ################################################
          access:
            # Possible values for 'scheme' are:
            #   1. http - force http to be used to connect to ClickHouse instances
            #   2. https - force https to be used to connect to ClickHouse instances
            #   3. auto - either http or https is selected based on open ports
            scheme: "auto"
            # ClickHouse credentials (username, password and port) to be used by the operator to connect to ClickHouse instances.
            # These credentials are used for:
            #   1. Metrics requests
            #   2. Schema maintenance
            #   3. DROP DNS CACHE
            # User with these credentials can be specified in additional ClickHouse .xml config files,
            # located in 'clickhouse.configuration.file.path.user' folder
            username: ""
            password: ""
            rootCA: ""
            # Location of the k8s Secret with username and password to be used by the operator to connect to ClickHouse instances.
            # Can be used instead of explicitly specified username and password available in sections:
            #   - clickhouse.access.username
            #   - clickhouse.access.password
            # Secret should have two keys:
            #   1. username
            #   2. password
            secret:
              # Empty `namespace` means that k8s secret would be looked in the same namespace where operator's pod is running.
              namespace: ""
              # Empty `name` means no k8s Secret would be looked for
              name: '{{ include "altinity-clickhouse-operator.fullname" . }}'
            # Port where to connect to ClickHouse instances to
            port: 8123
            # Timeouts used to limit connection and queries from the operator to ClickHouse instances
            # Specified in seconds.
            timeouts:
              # Timout to setup connection from the operator to ClickHouse instances. In seconds.
              connect: 1
              # Timout to perform SQL query from the operator to ClickHouse instances. In seconds.
              query: 4
          #################################################
          ##
          ## Metrics collection
          ##
          ################################################
          metrics:
            # Timeouts used to limit connection and queries from the metrics exporter to ClickHouse instances
            # Specified in seconds.
            timeouts:
              # Timeout used to limit metrics collection request. In seconds.
              # Upon reaching this timeout metrics collection is aborted and no more metrics are collected in this cycle.
              # All collected metrics are returned.
              collect: 9
        ################################################
        ##
        ## Template(s) management section
        ##
        ################################################
        template:
          chi:
            # CHI template updates handling policy
            # Possible policy values:
            #   - ReadOnStart. Accept CHIT updates on the operators start only.
            #   - ApplyOnNextReconcile. Accept CHIT updates at all time. Apply news CHITs on next regular reconcile of the CHI
            policy: ApplyOnNextReconcile
            # Path to the folder where ClickHouseInstallation templates .yaml manifests are located.
            # Templates are added to the list of all templates and used when CHI is reconciled.
            # Templates are applied in sorted alpha-numeric order.
            path: templates.d
        ################################################
        ##
        ## Reconcile section
        ##
        ################################################
        reconcile:
          # Reconcile runtime settings
          runtime:
            # Max number of concurrent CHI reconciles in progress
            reconcileCHIsThreadsNumber: 10
            # The operator reconciles shards concurrently in each CHI with the following limitations:
            #   1. Number of shards being reconciled (and thus having hosts down) in each CHI concurrently
            #      can not be greater than 'reconcileShardsThreadsNumber'.
            #   2. Percentage of shards being reconciled (and thus having hosts down) in each CHI concurrently
            #      can not be greater than 'reconcileShardsMaxConcurrencyPercent'.
            #   3. The first shard is always reconciled alone. Concurrency starts from the second shard and onward.
            # Thus limiting number of shards being reconciled (and thus having hosts down) in each CHI by both number and percentage

            # Max number of concurrent shard reconciles within one CHI in progress
            reconcileShardsThreadsNumber: 5
            # Max percentage of concurrent shard reconciles within one CHI in progress
            reconcileShardsMaxConcurrencyPercent: 50
          # Reconcile StatefulSet scenario
          statefulSet:
            # Create StatefulSet scenario
            create:
              # What to do in case created StatefulSet is not in 'Ready' after `reconcile.statefulSet.update.timeout` seconds
              # Possible options:
              # 1. abort - abort the process, do nothing with the problematic StatefulSet, leave it as it is,
              #    do not try to fix or delete or update it, just abort reconcile cycle.
              #    Do not proceed to the next StatefulSet(s) and wait for an admin to assist.
              # 2. delete - delete newly created problematic StatefulSet and follow 'abort' path afterwards.
              # 3. ignore - ignore an error, pretend nothing happened, continue reconcile and move on to the next StatefulSet.
              onFailure: ignore
            # Update StatefulSet scenario
            update:
              # How many seconds to wait for created/updated StatefulSet to be 'Ready'
              timeout: 300
              # How many seconds to wait between checks/polls for created/updated StatefulSet status
              pollInterval: 5
              # What to do in case updated StatefulSet is not in 'Ready' after `reconcile.statefulSet.update.timeout` seconds
              # Possible options:
              # 1. abort - abort the process, do nothing with the problematic StatefulSet, leave it as it is,
              #    do not try to fix or delete or update it, just abort reconcile cycle.
              #    Do not proceed to the next StatefulSet(s) and wait for an admin to assist.
              # 2. rollback - delete Pod and rollback StatefulSet to previous Generation.
              #    Pod would be recreated by StatefulSet based on rollback-ed StatefulSet configuration.
              #    Follow 'abort' path afterwards.
              # 3. ignore - ignore an error, pretend nothing happened, continue reconcile and move on to the next StatefulSet.
              onFailure: abort
          # Reconcile Host scenario
          host:
            # Whether the operator during reconcile procedure should wait for a ClickHouse host:
            #   - to be excluded from a ClickHouse cluster
            #   - to complete all running queries
            #   - to be included into a ClickHouse cluster
            # respectfully before moving forward
            wait:
              exclude: true
              queries: true
              include: false
        ################################################
        ##
        ## Annotations management section
        ##
        ################################################
        annotation:
          # Applied when:
          #  1. Propagating annotations from the CHI's `metadata.annotations` to child objects' `metadata.annotations`,
          #  2. Propagating annotations from the CHI Template's `metadata.annotations` to CHI's `metadata.annotations`,
          # Include annotations from the following list:
          # Applied only when not empty. Empty list means "include all, no selection"
          include: []
          # Exclude annotations from the following list:
          exclude: []
        ################################################
        ##
        ## Labels management section
        ##
        ################################################
        label:
          # Applied when:
          #  1. Propagating labels from the CHI's `metadata.labels` to child objects' `metadata.labels`,
          #  2. Propagating labels from the CHI Template's `metadata.labels` to CHI's `metadata.labels`,
          # Include labels from the following list:
          # Applied only when not empty. Empty list means "include all, no selection"
          include: []
          # Exclude labels from the following list:
          # Applied only when not empty. Empty list means "nothing to exclude, no selection"
          exclude: []
          # Whether to append *Scope* labels to StatefulSet and Pod.
          # Full list of available *scope* labels check in 'labeler.go'
          #  LabelShardScopeIndex
          #  LabelReplicaScopeIndex
          #  LabelCHIScopeIndex
          #  LabelCHIScopeCycleSize
          #  LabelCHIScopeCycleIndex
          #  LabelCHIScopeCycleOffset
          #  LabelClusterScopeIndex
          #  LabelClusterScopeCycleSize
          #  LabelClusterScopeCycleIndex
          #  LabelClusterScopeCycleOffset
          appendScope: "no"
        ################################################
        ##
        ## StatefulSet management section
        ##
        ################################################
        statefulSet:
          revisionHistoryLimit: 0
        ################################################
        ##
        ## Pod management section
        ##
        ################################################
        pod:
          # Grace period for Pod termination.
          # How many seconds to wait between sending
          # SIGTERM and SIGKILL during Pod termination process.
          # Increase this number is case of slow shutdown.
          terminationGracePeriod: 30
        ################################################
        ##
        ## Log parameters section
        ##
        ################################################
        logger:
          logtostderr: "true"
          alsologtostderr: "false"
          v: "1"
          stderrthreshold: ""
          vmodule: ""
          log_backtrace_at: ""
    templatesdFiles:
      001-templates.json.example: |
        {
          "apiVersion": "clickhouse.altinity.com/v1",
          "kind": "ClickHouseInstallationTemplate",
          "metadata": {
            "name": "01-default-volumeclaimtemplate"
          },
          "spec": {
            "templates": {
              "volumeClaimTemplates": [
                {
                  "name": "chi-default-volume-claim-template",
                  "spec": {
                    "accessModes": [
                      "ReadWriteOnce"
                    ],
                    "resources": {
                      "requests": {
                        "storage": "2Gi"
                      }
                    }
                  }
                }
              ],
              "podTemplates": [
                {
                  "name": "chi-default-oneperhost-pod-template",
                  "distribution": "OnePerHost",
                  "spec": {
                    "containers" : [
                      {
                        "name": "clickhouse",
                        "image": "clickhouse/clickhouse-server:23.8",
                        "ports": [
                          {
                            "name": "http",
                            "containerPort": 8123
                          },
                          {
                            "name": "client",
                            "containerPort": 9000
                          },
                          {
                            "name": "interserver",
                            "containerPort": 9009
                          }
                        ]
                      }
                    ]
                  }
                }
              ]
            }
          }
        }
      default-pod-template.yaml.example: |
        apiVersion: "clickhouse.altinity.com/v1"
        kind: "ClickHouseInstallationTemplate"
        metadata:
          name: "default-oneperhost-pod-template"
        spec:
          templates:
            podTemplates:
              - name: default-oneperhost-pod-template
                distribution: "OnePerHost"
      default-storage-template.yaml.example: |
        apiVersion: "clickhouse.altinity.com/v1"
        kind: "ClickHouseInstallationTemplate"
        metadata:
          name: "default-storage-template-2Gi"
        spec:
          templates:
            volumeClaimTemplates:
              - name: default-storage-template-2Gi
                spec:
                  accessModes:
                    - ReadWriteOnce
                  resources:
                    requests:
                      storage: 2Gi
      readme: |-
        Templates in this folder are packaged with an operator and available via 'useTemplate'
    usersdFiles:
      01-clickhouse-operator-profile.xml: |
        <!-- IMPORTANT -->
        <!-- This file is auto-generated -->
        <!-- Do not edit this file - all changes would be lost -->
        <!-- Edit appropriate template in the following folder: -->
        <!-- deploy/builder/templates-config -->
        <!-- IMPORTANT -->
        <!--
        #
        # Template parameters available:
        #
        -->
        <yandex>
            <!-- clickhouse-operator user is generated by the operator based on config.yaml in runtime -->
            <profiles>
                <clickhouse_operator>
                    <log_queries>0</log_queries>
                    <skip_unavailable_shards>1</skip_unavailable_shards>
                    <http_connection_timeout>10</http_connection_timeout>
                    <max_concurrent_queries_for_all_users>0</max_concurrent_queries_for_all_users>
                    <os_thread_priority>0</os_thread_priority>
                </clickhouse_operator>
            </profiles>
        </yandex>
      02-clickhouse-default-profile.xml: |-
        <!-- IMPORTANT -->
        <!-- This file is auto-generated -->
        <!-- Do not edit this file - all changes would be lost -->
        <!-- Edit appropriate template in the following folder: -->
        <!-- deploy/builder/templates-config -->
        <!-- IMPORTANT -->
        <yandex>
          <profiles>
            <default>
              <os_thread_priority>2</os_thread_priority>
              <log_queries>1</log_queries>
              <connect_timeout_with_failover_ms>1000</connect_timeout_with_failover_ms>
              <distributed_aggregation_memory_efficient>1</distributed_aggregation_memory_efficient>
              <parallel_view_processing>1</parallel_view_processing>
              <do_not_merge_across_partitions_select_final>1</do_not_merge_across_partitions_select_final>
              <load_balancing>nearest_hostname</load_balancing>
              <prefer_localhost_replica>0</prefer_localhost_replica>
              <!-- materialize_ttl_recalculate_only>1</materialize_ttl_recalculate_only> 21.10 and above -->
            </default>
          </profiles>
        </yandex>
  # additionalResources -- list of additional resources to create (are processed via `tpl` function), useful for create ClickHouse clusters together with clickhouse-operator, look `kubectl explain chi` for details
  additionalResources: []
  #  - |
  #    apiVersion: v1
  #    kind: ConfigMap
  #    metadata:
  #      name: {{ include "altinity-clickhouse-operator.fullname" . }}-cm
  #      namespace: {{ .Release.Namespace }}
  #  - |
  #     apiVersion: v1
  #     kind: Secret
  #     metadata:
  #       name: {{ include "altinity-clickhouse-operator.fullname" . }}-s
  #       namespace: {{ .Release.Namespace }}
  #     stringData:
  #       mykey: my-value
  #  - |
  #     apiVersion: clickhouse.altinity.com/v1
  #     kind: ClickHouseInstallation
  #     metadata:
  #       name: {{ include "altinity-clickhouse-operator.fullname" . }}-chi
  #       namespace: {{ .Release.Namespace }}
  #     spec:
  #       configuration:
  #         clusters:
  #         - name: default
  #           layout:
  #             shardsCount: 1
  dashboards:
    # dashboards.enabled -- provision grafana dashboards as secrets (can be synced by grafana dashboards sidecar https://github.com/grafana/helm-charts/blob/grafana-6.33.1/charts/grafana/values.yaml#L679 )
    enabled: false
    # dashboards.additionalLabels -- labels to add to a secret with dashboards
    additionalLabels:
      grafana_dashboard: ""
    # dashboards.annotations -- annotations to add to a secret with dashboards
    annotations: {}
    grafana_folder: clickhouse

zookeeper:
  enabled: true

  ## @param global.imageRegistry Global Docker image registry
  ## @param global.imagePullSecrets Global Docker registry secret names as an array
  ## @param global.storageClass Global StorageClass for Persistent Volume(s)
  ##
  global:
    imageRegistry: ""
    ## E.g.
    ## imagePullSecrets:
    ##   - myRegistryKeySecretName
    ##
    imagePullSecrets: []
    storageClass: ""
    ## Compatibility adaptations for Kubernetes platforms
    ##
    compatibility:
      ## Compatibility adaptations for Openshift
      ##
      openshift:
        ## @param global.compatibility.openshift.adaptSecurityContext Adapt the securityContext sections of the deployment to make them compatible with Openshift restricted-v2 SCC: remove runAsUser, runAsGroup and fsGroup and let the platform use their allowed default IDs. Possible values: auto (apply if the detected running cluster is Openshift), force (perform the adaptation always), disabled (do not perform adaptation)
        ##
        adaptSecurityContext: disabled
  ## @section Common parameters
  ##

  ## @param kubeVersion Override Kubernetes version
  ##
  kubeVersion: ""
  ## @param nameOverride String to partially override common.names.fullname template (will maintain the release name)
  ##
  nameOverride: ""
  ## @param fullnameOverride String to fully override common.names.fullname template
  ##
  fullnameOverride: ""
  ## @param clusterDomain Kubernetes Cluster Domain
  ##
  clusterDomain: cluster.local
  ## @param extraDeploy Extra objects to deploy (evaluated as a template)
  ##
  extraDeploy: []
  ## @param commonLabels Add labels to all the deployed resources
  ##
  commonLabels: {}
  ## @param commonAnnotations Add annotations to all the deployed resources
  ##
  commonAnnotations: {}
  ## @param namespaceOverride Override namespace for ZooKeeper resources
  ## Useful when including ZooKeeper as a chart dependency, so it can be released into a different namespace than the parent
  ##
  namespaceOverride: ""
  ## Enable diagnostic mode in the statefulset
  ##
  diagnosticMode:
    ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)
    ##
    enabled: false
    ## @param diagnosticMode.command Command to override all containers in the statefulset
    ##
    command:
      - sleep
    ## @param diagnosticMode.args Args to override all containers in the statefulset
    ##
    args:
      - infinity
  ## @section ZooKeeper chart parameters

  ## Bitnami ZooKeeper image version
  ## ref: https://hub.docker.com/r/bitnami/zookeeper/tags/
  ## @param image.registry [default: REGISTRY_NAME] ZooKeeper image registry
  ## @param image.repository [default: REPOSITORY_NAME/zookeeper] ZooKeeper image repository
  ## @skip image.tag ZooKeeper image tag (immutable tags are recommended)
  ## @param image.digest ZooKeeper image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ## @param image.pullPolicy ZooKeeper image pull policy
  ## @param image.pullSecrets Specify docker-registry secret names as an array
  ## @param image.debug Specify if debug values should be set
  ##
  image:
    registry: docker.io
    repository: bitnami/zookeeper
    tag: 3.9.1-debian-12-r15
    digest: ""
    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
    ##
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## Example:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
    ## Set to true if you would like to see extra information on logs
    ##
    debug: false
  ## Authentication parameters
  ##
  auth:
    client:
      ## @param auth.client.enabled Enable ZooKeeper client-server authentication. It uses SASL/Digest-MD5
      ##
      enabled: false
      ## @param auth.client.clientUser User that will use ZooKeeper clients to auth
      ##
      clientUser: ""
      ## @param auth.client.clientPassword Password that will use ZooKeeper clients to auth
      ##
      clientPassword: ""
      ## @param auth.client.serverUsers Comma, semicolon or whitespace separated list of user to be created
      ## Specify them as a string, for example: "user1,user2,admin"
      ##
      serverUsers: ""
      ## @param auth.client.serverPasswords Comma, semicolon or whitespace separated list of passwords to assign to users when created
      ## Specify them as a string, for example: "pass4user1, pass4user2, pass4admin"
      ##
      serverPasswords: ""
      ## @param auth.client.existingSecret Use existing secret (ignores previous passwords)
      ##
      existingSecret: ""
    quorum:
      ## @param auth.quorum.enabled Enable ZooKeeper server-server authentication. It uses SASL/Digest-MD5
      ##
      enabled: false
      ## @param auth.quorum.learnerUser User that the ZooKeeper quorumLearner will use to authenticate to quorumServers.
      ## Note: Make sure the user is included in auth.quorum.serverUsers
      ##
      learnerUser: ""
      ## @param auth.quorum.learnerPassword Password that the ZooKeeper quorumLearner will use to authenticate to quorumServers.
      ##
      learnerPassword: ""
      ## @param auth.quorum.serverUsers Comma, semicolon or whitespace separated list of users for the quorumServers.
      ## Specify them as a string, for example: "user1,user2,admin"
      ##
      serverUsers: ""
      ## @param auth.quorum.serverPasswords Comma, semicolon or whitespace separated list of passwords to assign to users when created
      ## Specify them as a string, for example: "pass4user1, pass4user2, pass4admin"
      ##
      serverPasswords: ""
      ## @param auth.quorum.existingSecret Use existing secret (ignores previous passwords)
      ##
      existingSecret: ""
  ## @param tickTime Basic time unit (in milliseconds) used by ZooKeeper for heartbeats
  ##
  tickTime: 2000
  ## @param initLimit ZooKeeper uses to limit the length of time the ZooKeeper servers in quorum have to connect to a leader
  ##
  initLimit: 10
  ## @param syncLimit How far out of date a server can be from a leader
  ##
  syncLimit: 5
  ## @param preAllocSize Block size for transaction log file
  ##
  preAllocSize: 65536
  ## @param snapCount The number of transactions recorded in the transaction log before a snapshot can be taken (and the transaction log rolled)
  ##
  snapCount: 100000
  ## @param maxClientCnxns Limits the number of concurrent connections that a single client may make to a single member of the ZooKeeper ensemble
  ##
  maxClientCnxns: 60
  ## @param maxSessionTimeout Maximum session timeout (in milliseconds) that the server will allow the client to negotiate
  ## Defaults to 20 times the tickTime
  ##
  maxSessionTimeout: 40000
  ## @param heapSize Size (in MB) for the Java Heap options (Xmx and Xms)
  ## This env var is ignored if Xmx an Xms are configured via `jvmFlags`
  ##
  heapSize: 1024
  ## @param fourlwCommandsWhitelist A list of comma separated Four Letter Words commands that can be executed
  ##
  fourlwCommandsWhitelist: srvr, mntr, ruok
  ## @param minServerId Minimal SERVER_ID value, nodes increment their IDs respectively
  ## Servers increment their ID starting at this minimal value.
  ## E.g., with `minServerId=10` and 3 replicas, server IDs will be 10, 11, 12 for z-0, z-1 and z-2 respectively.
  ##
  minServerId: 1
  ## @param listenOnAllIPs Allow ZooKeeper to listen for connections from its peers on all available IP addresses
  ##
  listenOnAllIPs: false
  ## Ongoing data directory cleanup configuration
  ##
  autopurge:
    ## @param autopurge.snapRetainCount The most recent snapshots amount (and corresponding transaction logs) to retain
    ##
    snapRetainCount: 10
    ## @param autopurge.purgeInterval The time interval (in hours) for which the purge task has to be triggered
    ## Set to a positive integer to enable the auto purging. Set to 0 to disable auto purging.
    ##
    purgeInterval: 1
  ## @param logLevel Log level for the ZooKeeper server. ERROR by default
  ## Have in mind if you set it to INFO or WARN the ReadinessProve will produce a lot of logs
  ##
  logLevel: ERROR
  ## @param jvmFlags Default JVM flags for the ZooKeeper process
  ##
  jvmFlags: ""
  ## @param dataLogDir Dedicated data log directory
  ## This allows a dedicated log device to be used, and helps avoid competition between logging and snapshots.
  ## E.g.
  ## dataLogDir: /bitnami/zookeeper/dataLog
  ##
  dataLogDir: ""
  ## @param configuration Configure ZooKeeper with a custom zoo.cfg file
  ## e.g:
  ## configuration: |-
  ##   deploy-working-dir=/bitnami/geode/data
  ##   log-level=info
  ##   ...
  ##
  configuration: ""
  ## @param existingConfigmap The name of an existing ConfigMap with your custom configuration for ZooKeeper
  ## NOTE: When it's set the `configuration` parameter is ignored
  ##
  existingConfigmap: ""
  ## @param extraEnvVars Array with extra environment variables to add to ZooKeeper nodes
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param extraEnvVarsCM Name of existing ConfigMap containing extra env vars for ZooKeeper nodes
  ##
  extraEnvVarsCM: ""
  ## @param extraEnvVarsSecret Name of existing Secret containing extra env vars for ZooKeeper nodes
  ##
  extraEnvVarsSecret: ""
  ## @param command Override default container command (useful when using custom images)
  ##
  command:
    - /scripts/setup.sh
  ## @param args Override default container args (useful when using custom images)
  ##
  args: []
  ## @section Statefulset parameters

  ## @param replicaCount Number of ZooKeeper nodes
  ##
  replicaCount: 1
  ## @param containerPorts.client ZooKeeper client container port
  ## @param containerPorts.tls ZooKeeper TLS container port
  ## @param containerPorts.follower ZooKeeper follower container port
  ## @param containerPorts.election ZooKeeper election container port
  ##
  containerPorts:
    client: 2181
    tls: 3181
    follower: 2888
    election: 3888
  ## Configure extra options for ZooKeeper containers' liveness, readiness and startup probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  ## @param livenessProbe.enabled Enable livenessProbe on ZooKeeper containers
  ## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param livenessProbe.successThreshold Success threshold for livenessProbe
  ## @param livenessProbe.probeCommandTimeout Probe command timeout for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
    probeCommandTimeout: 2
  ## @param readinessProbe.enabled Enable readinessProbe on ZooKeeper containers
  ## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param readinessProbe.successThreshold Success threshold for readinessProbe
  ## @param readinessProbe.probeCommandTimeout Probe command timeout for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
    probeCommandTimeout: 2
  ## @param startupProbe.enabled Enable startupProbe on ZooKeeper containers
  ## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param startupProbe.periodSeconds Period seconds for startupProbe
  ## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 15
    successThreshold: 1
  ## @param customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## @param lifecycleHooks for the ZooKeeper container(s) to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## ZooKeeper resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param resourcesPreset Set container resources according to one common preset (allowed values: none, nano, small, medium, large, xlarge, 2xlarge). This is ignored if resources is set (resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "none"
  ## @param resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param podSecurityContext.enabled Enabled ZooKeeper pods' Security Context
  ## @param podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param podSecurityContext.sysctls Set kernel settings using the sysctl interface
  ## @param podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param podSecurityContext.fsGroup Set ZooKeeper pod's Security Context fsGroup
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  ## @param containerSecurityContext.enabled Enabled containers' Security Context
  ## @param containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param containerSecurityContext.privileged Set container's Security Context privileged
  ## @param containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: null
    runAsUser: 1001
    runAsGroup: 0
    runAsNonRoot: true
    privileged: false
    readOnlyRootFilesystem: false
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## @param automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: false
  ## @param hostAliases ZooKeeper pods host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param podLabels Extra labels for ZooKeeper pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param podAnnotations Annotations for ZooKeeper pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Node affinity preset
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param nodeAffinityPreset.key Node label key to match Ignored if `affinity` is set.
    ## E.g.
    ## key: "kubernetes.io/e2e-az-name"
    ##
    key: ""
    ## @param nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set.
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param affinity Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## Note: podAffinityPreset, podAntiAffinityPreset, and nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}
  ## @param nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param tolerations Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## @param podManagementPolicy StatefulSet controller supports relax its ordering guarantees while preserving its uniqueness and identity guarantees. There are two valid pod management policies: `OrderedReady` and `Parallel`
  ## ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy
  ##
  podManagementPolicy: Parallel
  ## @param priorityClassName Name of the existing priority class to be used by ZooKeeper pods, priority class needs to be created beforehand
  ## Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  ##
  priorityClassName: ""
  ## @param schedulerName Kubernetes pod scheduler registry
  ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param updateStrategy.type ZooKeeper statefulset strategy type
  ## @param updateStrategy.rollingUpdate ZooKeeper statefulset rolling update configuration parameters
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}
  ## @param extraVolumes Optionally specify extra list of additional volumes for the ZooKeeper pod(s)
  ## Example Use Case: mount certificates to enable TLS
  ## e.g:
  ## extraVolumes:
  ## - name: zookeeper-keystore
  ##   secret:
  ##     defaultMode: 288
  ##     secretName: zookeeper-keystore
  ## - name: zookeeper-truststore
  ##   secret:
  ##     defaultMode: 288
  ##     secretName: zookeeper-truststore
  ##
  extraVolumes: []
  ## @param extraVolumeMounts Optionally specify extra list of additional volumeMounts for the ZooKeeper container(s)
  ## Example Use Case: mount certificates to enable TLS
  ## e.g:
  ## extraVolumeMounts:
  ## - name: zookeeper-keystore
  ##   mountPath: /certs/keystore
  ##   readOnly: true
  ## - name: zookeeper-truststore
  ##   mountPath: /certs/truststore
  ##   readOnly: true
  ##
  extraVolumeMounts: []
  ## @param sidecars Add additional sidecar containers to the ZooKeeper pod(s)
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## @param initContainers Add additional init containers to the ZooKeeper pod(s)
  ## Example:
  ## initContainers:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  initContainers: []
  ## ZooKeeper Pod Disruption Budget
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
  ## @param pdb.create Deploy a pdb object for the ZooKeeper pod
  ## @param pdb.minAvailable Minimum available ZooKeeper replicas
  ## @param pdb.maxUnavailable Maximum unavailable ZooKeeper replicas
  ##
  pdb:
    create: false
    minAvailable: ""
    maxUnavailable: 1
  ## @param enableServiceLinks Whether information about services should be injected into pod's environment variable
  ## The environment variables injected by service links are not used, but can lead to slow boot times or slow running of the scripts when there are many services in the current namespace.
  ## If you experience slow pod startups or slow running of the scripts you probably want to set this to `false`.
  ##
  enableServiceLinks: true
  ## DNS-Pod services
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
  ## @param dnsPolicy Specifies the DNS policy for the zookeeper pods
  ## DNS policies can be set on a per-Pod basis. Currently Kubernetes supports the following Pod-specific DNS policies.
  ## Available options: Default, ClusterFirst, ClusterFirstWithHostNet, None
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
  dnsPolicy: ""
  ## @param dnsConfig  allows users more control on the DNS settings for a Pod. Required if `dnsPolicy` is set to `None`
  ## The dnsConfig field is optional and it can work with any dnsPolicy settings.
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config
  ## E.g.
  ## dnsConfig:
  ##   nameservers:
  ##     - 192.0.2.1 # this is an example
  ##   searches:
  ##     - ns1.svc.cluster-domain.example
  ##     - my.dns.search.suffix
  ##   options:
  ##     - name: ndots
  ##       value: "2"
  ##     - name: edns0
  dnsConfig: {}
  ## @section Traffic Exposure parameters
  service:
    ## @param service.type Kubernetes Service type
    ##
    type: ClusterIP
    ## @param service.ports.client ZooKeeper client service port
    ## @param service.ports.tls ZooKeeper TLS service port
    ## @param service.ports.follower ZooKeeper follower service port
    ## @param service.ports.election ZooKeeper election service port
    ##
    ports:
      client: 2181
      tls: 3181
      follower: 2888
      election: 3888
    ## Node ports to expose
    ## NOTE: choose port between <30000-32767>
    ## @param service.nodePorts.client Node port for clients
    ## @param service.nodePorts.tls Node port for TLS
    ##
    nodePorts:
      client: ""
      tls: ""
    ## @param service.disableBaseClientPort Remove client port from service definitions.
    ##
    disableBaseClientPort: false
    ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param service.clusterIP ZooKeeper service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param service.loadBalancerIP ZooKeeper service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param service.loadBalancerSourceRanges ZooKeeper service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param service.externalTrafficPolicy ZooKeeper service external traffic policy
    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param service.annotations Additional custom annotations for ZooKeeper service
    ##
    annotations: {}
    ## @param service.extraPorts Extra ports to expose in the ZooKeeper service (normally used with the `sidecar` value)
    ##
    extraPorts: []
    ## @param service.headless.annotations Annotations for the Headless Service
    ## @param service.headless.publishNotReadyAddresses If the ZooKeeper headless service should publish DNS records for not ready pods
    ## @param service.headless.servicenameOverride String to partially override headless service name
    ##
    headless:
      publishNotReadyAddresses: true
      annotations: {}
      servicenameOverride: ""
  ## Network policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param networkPolicy.allowExternal Don't require client label for connections
    ## When set to false, only pods with the correct client label will have network access to the port Redis&reg; is
    ## listening on. When true, zookeeper accept connections from any source (with the correct destination port).
    ##
    allowExternal: true
    ## @param networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolice
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
    ## @param networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## @section Other Parameters

  ## Service account for ZooKeeper to use.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param serviceAccount.create Enable creation of ServiceAccount for ZooKeeper pod
    ##
    create: true
    ## @param serviceAccount.name The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
    ## Can be set to false if pods using this serviceAccount do not need to use K8s API
    ##
    automountServiceAccountToken: false
    ## @param serviceAccount.annotations Additional custom annotations for the ServiceAccount
    ##
    annotations: {}
  ## @section Persistence parameters

  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    ## @param persistence.enabled Enable ZooKeeper data persistence using PVC. If false, use emptyDir
    ##
    enabled: true
    ## @param persistence.existingClaim Name of an existing PVC to use (only when deploying a single replica)
    ##
    existingClaim: ""
    ## @param persistence.storageClass PVC Storage Class for ZooKeeper data volume
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param persistence.accessModes PVC Access modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size PVC Storage Request for ZooKeeper data volume
    ##
    size: 8Gi
    ## @param persistence.annotations Annotations for the PVC
    ##
    annotations: {}
    ## @param persistence.labels Labels for the PVC
    ##
    labels: {}
    ## @param persistence.selector Selector to match an existing Persistent Volume for ZooKeeper's data PVC
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: {}
    ## Persistence for a dedicated data log directory
    ##
    dataLogDir:
      ## @param persistence.dataLogDir.size PVC Storage Request for ZooKeeper's dedicated data log directory
      ##
      size: 8Gi
      ## @param persistence.dataLogDir.existingClaim Provide an existing `PersistentVolumeClaim` for ZooKeeper's data log directory
      ## If defined, PVC must be created manually before volume will be bound
      ## The value is evaluated as a template
      ##
      existingClaim: ""
      ## @param persistence.dataLogDir.selector Selector to match an existing Persistent Volume for ZooKeeper's data log PVC
      ## If set, the PVC can't have a PV dynamically provisioned for it
      ## E.g.
      ## selector:
      ##   matchLabels:
      ##     app: my-app
      ##
      selector: {}
  ## @section Volume Permissions parameters
  ##

  ## Init containers parameters:
  ## volumePermissions: Change the owner and group of the persistent volume(s) mountpoint(s) to 'runAsUser:fsGroup' on each node
  ##
  volumePermissions:
    ## @param volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume
    ##
    enabled: false
    ## @param volumePermissions.image.registry [default: REGISTRY_NAME] Init container volume-permissions image registry
    ## @param volumePermissions.image.repository [default: REPOSITORY_NAME/os-shell] Init container volume-permissions image repository
    ## @skip volumePermissions.image.tag Init container volume-permissions image tag (immutable tags are recommended)
    ## @param volumePermissions.image.digest Init container volume-permissions image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
    ## @param volumePermissions.image.pullPolicy Init container volume-permissions image pull policy
    ## @param volumePermissions.image.pullSecrets Init container volume-permissions image pull secrets
    ##
    image:
      registry: docker.io
      repository: bitnami/os-shell
      tag: 12-debian-12-r16
      digest: ""
      pullPolicy: IfNotPresent
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## Example:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## Init container resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param volumePermissions.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, small, medium, large, xlarge, 2xlarge). This is ignored if volumePermissions.resources is set (volumePermissions.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "none"
    ## @param volumePermissions.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Init container' Security Context
    ## Note: the chown of the data folder is done to containerSecurityContext.runAsUser
    ## and not the below volumePermissions.containerSecurityContext.runAsUser
    ## @param volumePermissions.containerSecurityContext.enabled Enabled init container Security Context
    ## @param volumePermissions.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param volumePermissions.containerSecurityContext.runAsUser User ID for the init container
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: null
      runAsUser: 0
  ## @section Metrics parameters
  ##

  ## ZooKeeper Prometheus Exporter configuration
  ##
  metrics:
    ## @param metrics.enabled Enable Prometheus to access ZooKeeper metrics endpoint
    ##
    enabled: false
    ## @param metrics.containerPort ZooKeeper Prometheus Exporter container port
    ##
    containerPort: 9141
    ## Service configuration
    ##
    service:
      ## @param metrics.service.type ZooKeeper Prometheus Exporter service type
      ##
      type: ClusterIP
      ## @param metrics.service.port ZooKeeper Prometheus Exporter service port
      ##
      port: 9141
      ## @param metrics.service.annotations [object] Annotations for Prometheus to auto-discover the metrics endpoint
      ##
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "{{ .Values.metrics.service.port }}"
        prometheus.io/path: "/metrics"
    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator
      ##
      enabled: false
      ## @param metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)
      ##
      namespace: ""
      ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: ""
      ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: ""
      ## @param metrics.serviceMonitor.additionalLabels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
      ##
      additionalLabels: {}
      ## @param metrics.serviceMonitor.selector Prometheus instance selector labels
      ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
      ##
      selector: {}
      ## @param metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
      ##
      relabelings: []
      ## @param metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
      ##
      metricRelabelings: []
      ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""
    ## Prometheus Operator PrometheusRule configuration
    ##
    prometheusRule:
      ## @param metrics.prometheusRule.enabled Create a PrometheusRule for Prometheus Operator
      ##
      enabled: false
      ## @param metrics.prometheusRule.namespace Namespace for the PrometheusRule Resource (defaults to the Release Namespace)
      ##
      namespace: ""
      ## @param metrics.prometheusRule.additionalLabels Additional labels that can be used so PrometheusRule will be discovered by Prometheus
      ##
      additionalLabels: {}
      ## @param metrics.prometheusRule.rules PrometheusRule definitions
      ##  - alert: ZooKeeperSyncedFollowers
      ##    annotations:
      ##      message: The number of synced followers for the leader node in ZooKeeper deployment my-release is less than 2. This usually means that some of the ZooKeeper nodes aren't communicating properly. If it doesn't resolve itself you can try killing the pods (one by one).
      ##    expr: max(synced_followers{service="my-release-metrics"}) < 2
      ##    for: 5m
      ##    labels:
      ##      severity: critical
      ##  - alert: ZooKeeperOutstandingRequests
      ##    annotations:
      ##      message: The number of outstanding requests for ZooKeeper pod {{ $labels.pod }} is greater than 10. This can indicate a performance issue with the Pod or cluster a whole.
      ##    expr: outstanding_requests{service="my-release-metrics"} > 10
      ##    for: 5m
      ##    labels:
      ##      severity: critical
      ##
      rules: []
  ## @section TLS/SSL parameters
  ##

  ## Enable SSL/TLS encryption
  ##
  tls:
    client:
      ## @param tls.client.enabled Enable TLS for client connections
      ##
      enabled: false
      ## @param tls.client.auth SSL Client auth. Can be "none", "want" or "need".
      ##
      auth: "none"
      ## @param tls.client.autoGenerated Generate automatically self-signed TLS certificates for ZooKeeper client communications
      ## Currently only supports PEM certificates
      ##
      autoGenerated: false
      ## @param tls.client.existingSecret Name of the existing secret containing the TLS certificates for ZooKeeper client communications
      ##
      existingSecret: ""
      ## @param tls.client.existingSecretKeystoreKey The secret key from the tls.client.existingSecret containing the Keystore.
      ##
      existingSecretKeystoreKey: ""
      ## @param tls.client.existingSecretTruststoreKey The secret key from the tls.client.existingSecret containing the Truststore.
      ##
      existingSecretTruststoreKey: ""
      ## @param tls.client.keystorePath Location of the KeyStore file used for Client connections
      ##
      keystorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.keystore.jks
      ## @param tls.client.truststorePath Location of the TrustStore file used for Client connections
      ##
      truststorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.truststore.jks
      ## @param tls.client.passwordsSecretName Existing secret containing Keystore and truststore passwords
      ##
      passwordsSecretName: ""
      ## @param tls.client.passwordsSecretKeystoreKey The secret key from the tls.client.passwordsSecretName containing the password for the Keystore.
      ##
      passwordsSecretKeystoreKey: ""
      ## @param tls.client.passwordsSecretTruststoreKey The secret key from the tls.client.passwordsSecretName containing the password for the Truststore.
      ##
      passwordsSecretTruststoreKey: ""
      ## @param tls.client.keystorePassword Password to access KeyStore if needed
      ##
      keystorePassword: ""
      ## @param tls.client.truststorePassword Password to access TrustStore if needed
      ##
      truststorePassword: ""
    quorum:
      ## @param tls.quorum.enabled Enable TLS for quorum protocol
      ##
      enabled: false
      ## @param tls.quorum.auth SSL Quorum Client auth. Can be "none", "want" or "need".
      ##
      auth: "none"
      ## @param tls.quorum.autoGenerated Create self-signed TLS certificates. Currently only supports PEM certificates.
      ##
      autoGenerated: false
      ## @param tls.quorum.existingSecret Name of the existing secret containing the TLS certificates for ZooKeeper quorum protocol
      ##
      existingSecret: ""
      ## @param tls.quorum.existingSecretKeystoreKey The secret key from the tls.quorum.existingSecret containing the Keystore.
      ##
      existingSecretKeystoreKey: ""
      ## @param tls.quorum.existingSecretTruststoreKey The secret key from the tls.quorum.existingSecret containing the Truststore.
      ##
      existingSecretTruststoreKey: ""
      ## @param tls.quorum.keystorePath Location of the KeyStore file used for Quorum protocol
      ##
      keystorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.keystore.jks
      ## @param tls.quorum.truststorePath Location of the TrustStore file used for Quorum protocol
      ##
      truststorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.truststore.jks
      ## @param tls.quorum.passwordsSecretName Existing secret containing Keystore and truststore passwords
      ##
      passwordsSecretName: ""
      ## @param tls.quorum.passwordsSecretKeystoreKey The secret key from the tls.quorum.passwordsSecretName containing the password for the Keystore.
      ##
      passwordsSecretKeystoreKey: ""
      ## @param tls.quorum.passwordsSecretTruststoreKey The secret key from the tls.quorum.passwordsSecretName containing the password for the Truststore.
      ##
      passwordsSecretTruststoreKey: ""
      ## @param tls.quorum.keystorePassword Password to access KeyStore if needed
      ##
      keystorePassword: ""
      ## @param tls.quorum.truststorePassword Password to access TrustStore if needed
      ##
      truststorePassword: ""
    ## Init container resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param tls.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, small, medium, large, xlarge, 2xlarge). This is ignored if tls.resources is set (tls.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "none"
    ## @param tls.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}